{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion recognition using NLP: Affective Computing 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danila Goncharenko, 2303788\n",
    "\n",
    "Ana Ferreira, 2308587\n",
    "\n",
    "Luca Hustiuc, 2209104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, pipeline, AutoTokenizer\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "model_nm = \"cardiffnlp/twitter-roberta-base\"\n",
    "fill_mask = pipeline(\"fill-mask\", model=model_nm, tokenizer=model_nm)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_nm)\n",
    "\n",
    "def print_candidates():\n",
    "    for i in range(5):\n",
    "        token = tokenizer.decode(candidates[i]['token'])\n",
    "        score = np.round(candidates[i]['score'], 4)\n",
    "        print(f\"{i+1}) {token} {score}\")\n",
    "\n",
    "texts = [\n",
    " \"I am so <mask> ðŸ˜Š\",\n",
    " \"I am so <mask> ðŸ˜¢\" \n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    t = preprocess(text)\n",
    "    print(f\"{'-'*30}\\n{t}\")\n",
    "    candidates = fill_mask(t)\n",
    "    print_candidates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import numpy as np\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "text = \"Good night ðŸ˜Š\"\n",
    "text = preprocess(text)\n",
    "\n",
    "# Pytorch\n",
    "model = AutoModel.from_pretrained(MODEL)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "features = model(**encoded_input)\n",
    "features = features[0].detach().cpu().numpy() \n",
    "features_mean = np.mean(features[0], axis=0) \n",
    "#features_max = np.max(features[0], axis=0)\n",
    "\n",
    "# # Tensorflow\n",
    "# model = TFAutoModel.from_pretrained(MODEL)\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# features = model(encoded_input)\n",
    "# features = features[0].numpy()\n",
    "# features_mean = np.mean(features[0], axis=0) \n",
    "# #features_max = np.max(features[0], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['neutral', 'worry', 'hapiness', 'sadness', 'love']\n",
    "\n",
    "def binarize_labels(labels):\n",
    "    return [int(i in list(map(int,labels[1:-1].split(',')))) for i in range(len(LABELS))]\n",
    "binarize_labels(df['labels'][0])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=len(LABELS), problem_type=\"multi_label_classification\")\n",
    "\n",
    "model.config.label2id = {label: i for i, label in enumerate(LABELS)}\n",
    "model.config.id2label = {i: label for i, label in enumerate(LABELS)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# validate\n",
    "# test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def get_pred(text):\n",
    "    inputs = tokz(text, return_tensors=\"pt\")\n",
    "    inputs.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    percent = nn.functional.softmax(logits, dim=1)\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    res = model.config.id2label[predicted_class_id]\n",
    "    return percent, predicted_class_id, res\n",
    "get_pred('This text has no emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "def predict_with_model(model, dataloader):\n",
    "    preds = []\n",
    "    facts = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        facts.append([list(map(bool,batch['label']))])\n",
    "        inputs = tokz(batch['text'], return_tensors=\"pt\")\n",
    "        inputs.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        preds.append(nn.functional.softmax(logits, dim=1).cpu())\n",
    "    facts = np.concatenate(facts)\n",
    "    preds = np.concatenate(preds)\n",
    "    return facts, preds\n",
    "def eval_model(preds, facts):\n",
    "    aucs = [roc_auc_score(facts[:, i], preds[:, i]) for i in range(len(LABELS))]\n",
    "    print('aucs:', aucs)\n",
    "    return {'accuracy': np.mean(aucs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factswm, predwm = predict_with_model(model, test_ds.select(range(100)))\n",
    "eval_model(predwm, factswm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds, facts = eval_pred\n",
    "    return eval_model(preds, facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5, \n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\", \n",
    "    metric_for_best_model = \"accuracy\", \n",
    "    load_best_model_at_end=True, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokz,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred('This text has no emotion.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred('Im afraid Ill fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred('Glad to see you!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred('I want to die')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred('OMG, so romantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "pd.DataFrame([\n",
    "    {av: f1_score(factswm[:, i], predwm[:, i] > 0.5, average=av) for av in ['binary', 'micro', 'macro']}\n",
    "    for i in range(len(LABELS))\n",
    "]).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    {av: f1_score(factswm[:, i], predwm[:, i] > 0.5, average=av) for av in ['binary', 'micro', 'macro']}\n",
    "    for i in range(len(LABELS))\n",
    "]).mean().round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
