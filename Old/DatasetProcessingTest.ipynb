{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a6acb3-5e7e-4a9c-83eb-4f7cc59987f2",
   "metadata": {},
   "source": [
    "In this notebook we will process the files inside the folder DatasetsInUse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614a627-0beb-4d5b-bcbe-e96b8aa6b94d",
   "metadata": {},
   "source": [
    "First, we will start to process the data from the emotion_tweets_2020 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "808100a6-f814-43f8-9c24-2d194d12c210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3257\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worry payment problem may never joyce meyer mo...</td>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roommate okay spell autocorrect terrible first...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cute atsu probably shy photos cherry help uwu</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rooneys fuck untouchable fuck dreadful depay l...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pretty depress u hit pan ur favourite highlighter</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotions\n",
       "0  worry payment problem may never joyce meyer mo...  optimism\n",
       "1  roommate okay spell autocorrect terrible first...     anger\n",
       "2      cute atsu probably shy photos cherry help uwu       joy\n",
       "3  rooneys fuck untouchable fuck dreadful depay l...     anger\n",
       "4  pretty depress u hit pan ur favourite highlighter   sadness"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def mapping(key_list):\n",
    "    emotion_map = {\n",
    "        '0': \"anger\",\n",
    "        '1': \"joy\",\n",
    "        '2': \"optimism\",\n",
    "        '3': \"sadness\",\n",
    "    }\n",
    "\n",
    "    emotions = [emotion_map[num] for num in key_list]\n",
    "\n",
    "    return emotions\n",
    "\n",
    "def specific_case(text):\n",
    "    result = re.sub(r'(&gt;){3}', 'is better than', text)\n",
    "    result = result.replace(\"szn\", \"season\")\n",
    "    result = re.sub(r'&[gl]t;?', '', result)\n",
    "    result = result.replace(\"ó\", \"o\")\n",
    "    result = result.replace(\"ñ\", \"n\")\n",
    "    result = result.replace(\"é\", \"e\")\n",
    "    return result\n",
    "\n",
    "def normalize_repeated_characters(text):\n",
    "    # Replace 3 or more consecutive characters with just one\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    return re.sub(r'@(\\w+)', r'\\1', text)\n",
    "\n",
    "def process_more_sign(text):\n",
    "    result = re.sub(r'\\s*user \\+', 'user', text)\n",
    "    result = re.sub(r'#\\++', '', result)\n",
    "    result = re.sub(r'(?<=\\d)\\+', ' more ', result)\n",
    "    result = re.sub(r'(?<=\\s)\\+', ' plus ', result)\n",
    "    result = re.sub(r'\\+1', ' plus one ', result)\n",
    "    return result\n",
    "\n",
    "def process_dollar(text):\n",
    "    result = re.sub(r'\\${2,}', 'cash', text)\n",
    "    pattern = r'\\$(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' dollars ', result)\n",
    "    result = re.sub(r'\\$*', '', result)\n",
    "    return result\n",
    "\n",
    "def process_euro(text):\n",
    "    pattern = r'\\€(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' euros ', text)\n",
    "    return result\n",
    "\n",
    "def process_pounds(text):\n",
    "    pattern = r'\\£(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' pounds ', text)\n",
    "    return result\n",
    "\n",
    "def process_percent(text):\n",
    "    pattern = r'(?:\\s+|\\d+(?:\\.\\d{0,2})?)%'\n",
    "    result = re.sub(pattern, lambda match: match.group(0).replace('%', ' percent '), text)\n",
    "    result = re.sub(r'%', '', result)\n",
    "    return result\n",
    "\n",
    "def process_equal(text):\n",
    "    result = re.sub(r'=', ' equals ', text)\n",
    "    return result\n",
    "\n",
    "def process_at(text):\n",
    "    result = re.sub(r'(?<=\\s)@(?=\\s)', ' at ', text)\n",
    "    return result\n",
    "\n",
    "def remove_newlines(text):\n",
    "    return re.sub(r'\\\\n', ' ', text)\n",
    "\n",
    "def process_amp(text):\n",
    "    return re.sub(r'&amp;?', ' and ', text)\n",
    "\n",
    "def process_hyphen(text):\n",
    "    return re.sub(r'(\\d+)\\s*-\\s*(\\d+)', r'\\1 to \\2', text)\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    p = inflect.engine()\n",
    "\n",
    "    number_pattern = r'(\\d+\\.\\d+|\\d+)'\n",
    "\n",
    "    numbers = re.findall(number_pattern, text)\n",
    "\n",
    "    for number in numbers:\n",
    "        word_representation = p.number_to_words(number)\n",
    "        text = re.sub(re.escape(number), word_representation, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clear_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join(lemmatizer.lemmatize(word, wordnet.VERB) for word in tokens)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join(word for word in tokens if word.lower() not in stop_words)\n",
    "\n",
    "def process_text(text):\n",
    "    text = specific_case(text)\n",
    "    text = remove_user_mentions(text)\n",
    "    text = process_more_sign(text)\n",
    "    text = process_dollar(text)\n",
    "    text = process_euro(text)\n",
    "    text = process_pounds(text)\n",
    "    text = process_percent(text)\n",
    "    text = process_hyphen(text)\n",
    "    text = process_equal(text)\n",
    "    text = process_at(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = process_amp(text)\n",
    "    text = replace_numbers_with_words(text)\n",
    "    text = normalize_repeated_characters(text)\n",
    "    text = clear_special_characters(text)\n",
    "    text = lowercase_text(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "def create_df(keys_file, values_file):\n",
    "    keys = read_file(keys_file)\n",
    "    keys = mapping(keys)    \n",
    "    values = read_file(values_file)\n",
    "\n",
    "    processed_values = [process_text(value) for value in values]\n",
    "\n",
    "    #for value, processed_value in zip(values, processed_values):\n",
    "        #print(f\"Original Value: {value} - Processed Value: {processed_value}\")\n",
    "\n",
    "    # Create a dictionary to store the data\n",
    "    data_dict = {\n",
    "        \"text\": processed_values,\n",
    "        \"emotions\": keys  # Use the processed values\n",
    "    }\n",
    "\n",
    "    # Create a Pandas DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    # df.head()\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_dictionary(keys_file, values_file):\n",
    "    keys = read_file(keys_file)\n",
    "    keys = mapping(keys)    \n",
    "    values = read_file(values_file)\n",
    "    \n",
    "    # print(keys[:5])\n",
    "\n",
    "    # print(values[2839])\n",
    "\n",
    "    if len(keys) != len(values):\n",
    "        print(\"Error: The number of keys and values does not match.\")\n",
    "        return None\n",
    "\n",
    "    non_alphanumeric_characters = {}\n",
    "    \n",
    "    for key, value in zip(keys, values):\n",
    "        \n",
    "        #if key == \"Optimism\":\n",
    "        #    print(value)\n",
    "        # Use a regular expression to find non-alphanumeric characters\n",
    "        #ats = []\n",
    "        #ats = re.findall(r\"(\\d)\\1{2,}\", value)\n",
    "        characters = re.findall(r'[^a-zA-Z0-9\\s]', value)\n",
    "        for character in characters:\n",
    "            if character not in non_alphanumeric_characters:\n",
    "                non_alphanumeric_characters[character] = value\n",
    "\n",
    "        #if ats != []:\n",
    "            #print(value)\n",
    "\n",
    "    print(\"Non-alphanumeric characters in the text:\")\n",
    "    for ch, sent in non_alphanumeric_characters.items():\n",
    "        print(ch)\n",
    "        print(sent)\n",
    "\n",
    "    return emotion_tweets\n",
    "\n",
    "keys_file = \"DatasetsInUse/emotion_tweets_2020/train_labels.txt\"\n",
    "values_file = \"DatasetsInUse/emotion_tweets_2020/train_text.txt\"\n",
    "\n",
    "df = create_df(keys_file, values_file)\n",
    "\n",
    "print(df.shape[0])\n",
    "\n",
    "df.head()\n",
    "\n",
    "# resulting_dict = create_dictionary(keys_file, values_file)\n",
    "\n",
    "#for key in list(resulting_dict.keys())[:3]:\n",
    "    #value = resulting_dict[key]\n",
    "    #print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1662d8-663f-48d8-a7bd-e4dc1d00b258",
   "metadata": {},
   "source": [
    "Secondly, we will process the data from the emotion folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d018a5-a7a9-4dfe-b00b-d9fed585b501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness' 'joy' 'love' 'anger' 'fear' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_pickle(\"DatasetsInUse/emotion/merged_training.pkl\")\n",
    "\n",
    "non_alphanumeric_characters = {}\n",
    "\n",
    "for row in df.itertuples():\n",
    "    characters = re.findall(r'[^a-zA-Z0-9\\s]', row.text)\n",
    "    for character in characters:\n",
    "            if character not in non_alphanumeric_characters:\n",
    "                non_alphanumeric_characters[character] = row.text\n",
    "    #print(row.emotions, row.text)\n",
    "\n",
    "# print(\"Non-alphanumeric characters in the text:\")\n",
    "for ch, sent in non_alphanumeric_characters.items():\n",
    "    print(ch)\n",
    "    print(sent)\n",
    "\n",
    "print(df.emotions.unique())\n",
    "#df.head()\n",
    "#print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16fdcd9d-4070-4cd9-b73a-e0f2021346f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so happy today!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_repeated_characters(text):\n",
    "    # Replace 3 or more consecutive characters with just one\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "# Example\n",
    "text = \"I'm sooooo happyyyy today!!!\"\n",
    "normalized_text = normalize_repeated_characters(text)\n",
    "\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ca9a2e0-51f0-44eb-a55f-1099b0d71356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey user, great post!\n"
     ]
    }
   ],
   "source": [
    "def remove_user_mentions(text):\n",
    "    return re.sub(r'@\\w+', 'user', text)\n",
    "\n",
    "# Example\n",
    "text = \"Hey @user, great post!\"\n",
    "cleaned_text = remove_user_mentions(text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7eb890eb-a60d-4c89-96a0-e82b0523f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Twitter     5 NLP\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "# Example\n",
    "text = \"Hello, Twitter! £§£€5 #NLP\"\n",
    "cleaned_text = remove_special_characters(text)\n",
    "cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', cleaned_text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5c2d3401-7047-4df2-8f89-4b10b73c2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: bdjdb berbéu é jhdbé éhdhdh ggdéff\n",
      "Modified Text: bdjdb berbeu e jhdbe ehdhdh ggdeff\n"
     ]
    }
   ],
   "source": [
    "def replace_szn_with_season(text):\n",
    "    # Use the str.replace() method to replace \"szn\" with \"season\"\n",
    "    #result = text.replace(\"szn\", \"season\")\n",
    "    result = text.replace(\"é\", \"e\")\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "original_text = \"@user szn 3 &gt;&gt;&gt; szn 1 &gt;&gt;&gt; szn 2. Just to warn you. Don't let szn 2 discourage you. \"\n",
    "text = \"bdjdb berbéu é jhdbé éhdhdh ggdéff\"\n",
    "modified_text = replace_szn_with_season(text)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Modified Text:\", modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c38eab88-7020-464c-89a7-4a42672bf01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: @user szn 3 &gt;&gt;&gt; szn 1 &gt;&gt;&gt; szn 2. Just to warn you. Don't let szn 2 discourage you. \n",
      "Modified Text: @user szn 3 is better than szn 1 is better than szn 2. Just to warn you. Don't let szn 2 discourage you. \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_is_better_than(text):\n",
    "    # Use re.sub to replace \"is better than\" with \">>>\"\n",
    "    result = re.sub(r'(&gt;){3}', 'is better than', text)\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "original_text = \"@user szn 3 &gt;&gt;&gt; szn 1 &gt;&gt;&gt; szn 2. Just to warn you. Don't let szn 2 discourage you. \"\n",
    "modified_text = replace_is_better_than(original_text)\n",
    "\n",
    "print(\"Original Text:\", original_text)\n",
    "print(\"Modified Text:\", modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d6432d92-f13b-494d-a899-153c23922d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user account created\n",
      "user account created\n",
      "user account created\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_user_plus_at_beginning(text):\n",
    "    pattern = r'\\s*user \\+'\n",
    "    replacement = 'user'\n",
    "    result = re.sub(pattern, replacement, text)\n",
    "    return result\n",
    "\n",
    "# Examples\n",
    "text1 = ' user + account created'\n",
    "text2 = 'user + account created'\n",
    "text3 = '    user + account created'\n",
    "\n",
    "print(replace_user_plus_at_beginning(text1))  # 'user account created'\n",
    "print(replace_user_plus_at_beginning(text2))  # 'user+ account created'\n",
    "print(replace_user_plus_at_beginning(text3))  # 'user account created'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "848a23bb-aa3c-4e9b-81b1-feff75042478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$$', '$$$', '$$$$']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_more_than_one_dollar_signs(text):\n",
    "    pattern = r'\\${2,}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "# Example\n",
    "text = \"The price is $$10.00, $$$25.50, and $$$$50.75.\"\n",
    "result = find_more_than_one_dollar_signs(text)\n",
    "print(result)  # ['$$', '$$$', '$$$$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d312579e-eab1-480d-8aac-61bbefaf47c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price is 10.00 dollars, but the total is 50.75 dollars.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def add_dollars_to_numbers(text):\n",
    "    # Define a regular expression pattern to match a dollar sign followed by a number\n",
    "    pattern = r'\\$(\\d+(?:\\.\\d{2})?)'\n",
    "    \n",
    "    # Use re.sub with a lambda function to replace matched text\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' dollars', text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example\n",
    "text = \"The price is $10.00, but the total is $50.75.\"\n",
    "result = add_dollars_to_numbers(text)\n",
    "print(result)  # \"The price is 10.00 dollars, but the total is 50.75 dollars.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "01c2a9d7-3b68-483d-a4ec-903eb8d07602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The interest rate is 5 percent , and the discount is 10.25  percent .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_percentages_with_string(text):\n",
    "    pattern = r'(?:\\s+|\\d+(?:\\.\\d{0,2})?)%'\n",
    "    \n",
    "    # Use re.sub with a lambda function to replace matched text\n",
    "    result = re.sub(pattern, lambda match: match.group(0).replace('%', ' percent '), text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example\n",
    "text = \"The interest rate is 5%, and the discount is 10.25 %.\"\n",
    "result = replace_percentages_with_string(text)\n",
    "print(result)  # \"The interest rate is 5 percent, and the discount is 10.25 percent.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8605fb8f-9ea3-4966-a33b-85f1ae88afb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' @ ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Send me an email at user@example.com. Mention me @ mention if you have questions.\"\n",
    "\n",
    "matches = re.findall(r'\\s@\\s', text)\n",
    "\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fc31c2f7-ba99-4c3d-9123-8493cf13462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send me an email at userexample.com. Mention me mention if you have questions.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Send me an email at user@example.com. Mention me @mention if you have questions.\"\n",
    "\n",
    "# Use a capturing group to capture the words after \"@\" and replace the entire match\n",
    "result = re.sub(r'@(\\w+)', r'\\1', text)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cf8a93f4-fc50-431f-bab2-72464ea7f664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send me an email at user  at  example.com. Mention me  at  mention if you have questions.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def process_at(text):\n",
    "    result = re.sub(r'(?<=\\s)@(?=\\s)', ' at ', text)\n",
    "    # result = re.sub(r'(?:\\s)(@)(?:\\s)', lambda match: match.group(1).replace('@', ' at '), text)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "text = \"Send me an email at user @ example.com. Mention me @ mention if you have questions.\"\n",
    "\n",
    "result = process_at(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "848de315-c2f1-454b-bc89-bd6eaa51e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I be run in the park .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join(lemmatizer.lemmatize(word, wordnet.VERB) for word in tokens)\n",
    "\n",
    "# Example\n",
    "text = \"I am                running in        the           park               .\"\n",
    "lemmatized_text = lemmatize_text(text)\n",
    "\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b75c69b8-5109-4f7e-881a-5a7896d4e3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I AM RACHEL</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOU ARE OUT OF YOUR MIND</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LALALALA</td>\n",
       "      <td>Optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO MORE</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YAY</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text  emotions\n",
       "0               I AM RACHEL     Anger\n",
       "1  YOU ARE OUT OF YOUR MIND       Joy\n",
       "2                  LALALALA  Optimism\n",
       "3                   NO MORE   Sadness\n",
       "4                       YAY       Joy"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "keys = [\"Anger\", \"Joy\", \"Optimism\", \"Sadness\", \"Joy\", \"Joy\"]\n",
    "values = [\"I am Rachel\", \"You are out of your mind\", \"Lalalala\", \"No more\", \"Yay\", \"Let's go!\"]\n",
    "\n",
    "# Define a function to process each element in the \"Values\" list\n",
    "def process_value(value):\n",
    "    # Apply your processing function here\n",
    "    return value.upper()  # For example, convert to uppercase\n",
    "\n",
    "# Apply the processing function to each element in the \"Values\" list\n",
    "processed_values = [process_value(value) for value in values]\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "data_dict = {\n",
    "    \"text\": processed_values,\n",
    "    \"emotions\": keys  # Use the processed values\n",
    "}\n",
    "\n",
    "# Create a Pandas DataFrame from the dictionary\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1798a02-60cd-4e3a-9118-de27f2ff7aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "{0: 'admiration', 1: 'amusement', 2: 'anger', 3: 'annoyance', 4: 'approval', 5: 'caring', 6: 'confusion', 7: 'curiosity', 8: 'desire', 9: 'disappointment', 10: 'disapproval', 11: 'disgust', 12: 'embarrassment', 13: 'excitement', 14: 'fear', 15: 'gratitude', 16: 'grief', 17: 'joy', 18: 'love', 19: 'nervousness', 20: 'optimism', 21: 'pride', 22: 'realization', 23: 'relief', 24: 'remorse', 25: 'sadness', 26: 'surprise', 27: 'neutral'}\n",
      "{'anger': ['anger', 'annoyance', 'disapproval'], 'disgust': ['disgust'], 'fear': ['fear', 'nervousness'], 'joy': ['joy', 'amusement', 'approval', 'excitement', 'gratitude', 'love', 'optimism', 'relief', 'pride', 'admiration', 'desire', 'caring'], 'sadness': ['sadness', 'disappointment', 'embarrassment', 'grief', 'remorse'], 'surprise': ['surprise', 'realization', 'confusion', 'curiosity']}\n",
      "{('anger', 'annoyance', 'disapproval'): 'anger', ('disgust',): 'disgust', ('fear', 'nervousness'): 'fear', ('joy', 'amusement', 'approval', 'excitement', 'gratitude', 'love', 'optimism', 'relief', 'pride', 'admiration', 'desire', 'caring'): 'joy', ('sadness', 'disappointment', 'embarrassment', 'grief', 'remorse'): 'sadness', ('surprise', 'realization', 'confusion', 'curiosity'): 'surprise'}\n",
      "['joy', 'joy', 'anger', 'anger', 'joy', 'joy', 'surprise', 'surprise', 'joy', 'sadness', 'anger', 'disgust', 'sadness', 'joy', 'fear', 'joy', 'sadness', 'joy', 'joy', 'fear', 'joy', 'joy', 'surprise', 'joy', 'sadness', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "column_names = ['Text', 'Emotion', 'ID']\n",
    "\n",
    "# Replace 'your_file.tsv' with the path to your TSV file\n",
    "df = pd.read_csv('DatasetsInUse/go_emotion/train.tsv', delimiter='\\t', names=column_names)\n",
    "\n",
    "# function to divide each emotion string into a list of numbers\n",
    "# after that, the numbers are mapped to the corresponding emotion in emotions.txt\n",
    "# then these emotions are grouped\n",
    "\n",
    "with open('DatasetsInUse/go_emotion/emotions.txt', 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    file_contents = file.read()\n",
    "\n",
    "lines = file_contents.split('\\n')\n",
    "\n",
    "print(lines)\n",
    "\n",
    "map = {index: line for index, line in enumerate(lines)}\n",
    "\n",
    "print(map)\n",
    "\n",
    "# Specify the path to your local JSON file\n",
    "file_path = 'DatasetsInUse/go_emotion/ekman_mapping.json'\n",
    "\n",
    "try:\n",
    "    # Open the JSON file for reading\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        # Load and parse the JSON data\n",
    "        ekman_map = json.load(json_file)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f'JSON file not found: {file_path}')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {str(e)}')\n",
    "\n",
    "print(ekman_map)\n",
    "\n",
    "# Create a reverse lookup dictionary\n",
    "ekman_map = {tuple(value): key for key, value in ekman_map.items()}\n",
    "\n",
    "print(ekman_map)\n",
    "\n",
    "# ekman mapping of the values\n",
    "\n",
    "ekman_list = [value for emotion in lines for keys, value in ekman_map.items() if emotion in keys]\n",
    "\n",
    "print(ekman_list)\n",
    "\n",
    "def emotion_mapping(emotions, emotion_map, ekman_map):\n",
    "    numbers_list = emotions.split(',')\n",
    "    emotions_list = [emotion_map[num] for num in numbers_list]\n",
    "    # group the emotions\n",
    "    ekman_list = [value for emotion in emotions_list for keys, value in ekman_map.items() if emotion in keys]\n",
    "    duplicates = [(item, ekman_list.count(item)) for item in set(ekman_list) if ekman_list.count(item) > 1]\n",
    "    sorted_duplicates = sorted(duplicates, key=lambda x: x[1], reverse=True)\n",
    "    if sorted_duplicates == []:\n",
    "        return ekman_list[0]\n",
    "    else:\n",
    "        return sorted_duplicates[0][0]\n",
    "\n",
    "# Now, 'df' contains your data in a DataFrame\n",
    "#print(df['Emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8e6148-b5a8-447a-9522-8d0e578d6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 43410\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_file.txt' with the path to your file\n",
    "file_path = 'DatasetsInUse/go_emotion/train.tsv'\n",
    "\n",
    "# Initialize a line count variable\n",
    "line_count = 0\n",
    "\n",
    "# Open the file and count lines\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line_count += 1\n",
    "\n",
    "# Print the total number of lines\n",
    "print(\"Total number of lines:\", line_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1f5e853-a5cb-47f5-908b-765972ae9f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[(4, 5), (2, 3)]\n"
     ]
    }
   ],
   "source": [
    "my_list = [4, 2, 4, 2, 4, 4, 2, 4]\n",
    "\n",
    "# Find duplicates and create a list of tuples with element and count\n",
    "duplicates = [(item, my_list.count(item)) for item in set(my_list) if my_list.count(item) > 1]\n",
    "\n",
    "sorted_duplicates = sorted(duplicates, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(sorted_duplicates[0][0])\n",
    "\n",
    "print(sorted_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "77a49b36-86af-4919-ac47-864530e720a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'anger', 'fear', 'surprise', 'joy', 'sadness',\n",
       "       'disgust'], dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "column_names = ['Text', 'Emotion', 'ID']\n",
    "\n",
    "df = pd.read_csv('DatasetsInUse/go_emotion/train.tsv', delimiter='\\t', names=column_names)\n",
    "\n",
    "with open('DatasetsInUse/go_emotion/emotions.txt', 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    file_contents = file.read()\n",
    "\n",
    "lines = file_contents.split('\\n')\n",
    "\n",
    "map = {index: line for index, line in enumerate(lines)}\n",
    "\n",
    "file_path = 'DatasetsInUse/go_emotion/ekman_mapping.json'\n",
    "\n",
    "try:\n",
    "    # Open the JSON file for reading\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        # Load and parse the JSON data\n",
    "        ekman_map = json.load(json_file)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f'JSON file not found: {file_path}')\n",
    "except Exception as e:\n",
    "    print(f'An error occurred: {str(e)}')\n",
    "\n",
    "# Create a reverse lookup dictionary\n",
    "ekman_map = {tuple(value): key for key, value in ekman_map.items()}\n",
    "\n",
    "# function to divide each emotion string into a list of numbers\n",
    "# after that, the numbers are mapped to the corresponding emotion in emotions.txt\n",
    "# then these emotions are grouped\n",
    "def emotion_mapping(emotions, emotion_map, ekman_map):\n",
    "    numbers_list = emotions.split(',')\n",
    "    #print(numbers_list)\n",
    "    emotions_list = [emotion_map[int(num)] for num in numbers_list]\n",
    "    #print(emotions_list)\n",
    "    # group the emotions\n",
    "    ekman_list = [value for emotion in emotions_list for keys, value in ekman_map.items() if emotion in keys]\n",
    "    duplicates = [(item, ekman_list.count(item)) for item in set(ekman_list) if ekman_list.count(item) > 1]\n",
    "    sorted_duplicates = sorted(duplicates, key=lambda x: x[1], reverse=True)\n",
    "    #print(ekman_list)\n",
    "    #print(sorted_duplicates)\n",
    "    if sorted_duplicates == []:\n",
    "        if ekman_list == []:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return ekman_list[0]\n",
    "    else:\n",
    "        return sorted_duplicates[0][0]\n",
    "\n",
    "df['Emotion'] = df['Emotion'].apply(emotion_mapping, args = (map, ekman_map))\n",
    "df.shape[0]\n",
    "df.Emotion.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "422ca742-bfe9-4ce9-9078-2d8203cf2823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think the 90 day rule applies to increases over 5%?\n",
      "100%. She's made herself vulnerable to being sued by a few people.\n",
      "If only. He’s signing with the red wings 100%.\n",
      "Deadlift and 3-lift total 100% raw federation\n",
      "100%,I'm actually impressed\n",
      "Be careful, or he'll have to use 10% of his power on you!\n",
      "I’m glad you fixed Minnesota, because there is a 0% chance we will go Red.\n",
      "As high as ~72% in black families. Truly heartbreaking what the expansion of the welfare state has done to the black family.\n",
      "Almost the same Win% as [NAME]. Nice.\n",
      "Fuck that its not my money they're giving him. I want a goalie with a .970 sv% dangnabbit.\n",
      "He's worth it I think. Dude brings it 110% every game\n",
      "Just a quick correction: if the next tax bracket is 10% than $10001 would be taxed $500.10\n",
      "I used to have anxiety when calling people but I no longer have them at least just 20-30% anxiety.\n",
      "Absolutely. 100%.\n",
      "All the things you mentioned are basically aligned perfectly in 99% of cases.\n",
      "I switched to Ally and love it. 2.2% interest is awesome\n",
      "No, it counts for 50% of the voting. We have to see what the players, coaches, and media vote for.\n",
      "100% communication with an NT is awesome!\n",
      "Yeah guys assume 95% of attractive girls are taken OR have a bunch of guys hitting on them online.\n",
      "Facebook is currently under fire for allegedly inflating their user base statistics by up to 50%. I'm not surprised they are having issues tracking money.\n",
      "Apologies. My point still stands. A .8% lead is an insignificant majority at best. Saying one demograohic is most of the population is a bit misleading\n",
      "I have an 88% wr rn as [NAME].\n",
      "50% of this sub is \"lul u don't have sex\"\n",
      "Puppies in 95% of cases. Sorry but some dogs are ugly no matter how old they are.\n",
      "of moms give birth BY 30. About 11-13% of women are childless. Please lol some more.\n",
      "I firmly believe that at least 30% people on that subs are just trolls to make the other 70% look like idiots.\n",
      "Thank [NAME] this was said, I feel 100% the same way. \n",
      "bargaining chip for what? The 25% tariff increase is not big enough of a bargaining chip? lol\n",
      "I got offered a couple of batches. They weren't 100% terrible but I feel like they could've been better so I didn't take them.\n",
      "Now only up 250,000%!\n",
      "Thats about how 10% of the US population are lol\n",
      "I have a feeling this is 99.9% of us.\n",
      "So many? I’d be amazed if it was more than 0.1% of active players at any time\n",
      "Even after achieving 100% in this game, his performance still amazes me.\n",
      "> Locking someone in a cage is 100% incompatible with a world where all violence not commited in self defense is immoral. Morals are subjective\n",
      "I always thought a visible/sortable +/- x% ranking would be interesting for that reason.\n",
      "He might be bummed but if I could play anywhere other than Toronto my next choice would 100% be LA.\n",
      "100%. Do NOT order from there. I have the same story. Don’t post that place on reddit anymore. They’re a scam\n",
      ">Not all 70% of the filthy [NAME] money is gonna be taken. ...unfortunately.\n",
      "No you basically throwing him under the bus by posting him here.... fake natty suspicion 100%\n",
      "I can see it, the wig/bodysuit is very kalorie but the makeup is still 100% India\n",
      "You could weigh even more than 152 at 10% 6 ft tall! But yes, that is the goal physique for many gym rats. \n",
      "I think they mean the banker is being unethical, but I'm not 100% sure\n",
      "If someone has tithed 10% their entire life, fuck off with requests like this.\n",
      "The real worst though is when she says, \"I love you more\" 100% seriously.\n",
      "43% on a one bedroom apartment (with a nice large balcony) in Potts Point. Sucks to be single.\n",
      "Have you told him you aren't 100% ok with it? It sounds like you're regretting this decision.\n",
      "Yeah but how is that even possible if 99% of girls who are out are with friends? That’s pretty intimidating\n",
      "go on, it's not like your Ni is, 100% accurate, what is my type?\n",
      "> While less than 1% of DV accusations end up being unsubstantiated I find that incredible.\n",
      "I will bet you $1000 this drags on until 2020 because [NAME] is 100% the naked partisan hack he is purported to be.\n",
      "Lol 100% science and you are defending her honor\n",
      "Now take this, cut out 100% of the killing and you have the Swiss Army. \n",
      "No idea man. Go check that one “study” done thirty years ago where you got that 40% from.\n",
      "All I'm saying is there is no way I would let one solo drop my whole team with a quad 0% chance\n",
      "Where did you get that? Highest I’ve found is Donagal at 23%.\n",
      "Hold on.... Why are we using OKCupid anyway. Wasn’t it basically proven that more than 50% of its user base is fake.\n",
      "Fortunately, those defibs only require being able to read to operate, so that means a good 5% of most corporate drones could run one.\n",
      "Came here to say that. I'm genuinely astounded that it's as much as 12%.\n",
      "Crazy stuff! Gets 80% to shatter before taking any damage. That positioning is on point!\n",
      "i just don’t think we are aggressive 100% of the time\n",
      "How are you 100% positive that you're pregnant if you haven't taken a test yet?\n",
      "The prevailing expectation is that Hulu will become 100% Disney owned in the future.\n",
      "Bro, put water in the microwave and see what happens, we're like 70% that shit\n",
      "Agreed 100%, Governor [NAME] will have some say in this, they broke Wisconsin law, plain and simple !\n",
      "It's 57% upvoted, which pretty much means most people hate it. \n",
      "Yes it is I’m 99.9% sure! Thank you! Enjoy your gold.\n",
      "I guess the trick is not to eliminate susceptibility to 51% but to minimize it extremely.\n",
      "I absolutely agree 100%.\n",
      "It’s really 90%, which is what the customer is told. Corporate isn’t real smart when it comes to signage. \n",
      "According to Skyrim, me and 90% of the population are inner stealth archers.\n",
      "Yeeeeaaaaahhhhh, im gonna need a source on that 40% statistic\n",
      "Just in case you weren’t sure he’s an [NAME], he decided to obstruct 20% of his windshield to remind you. \n",
      "What would that do? So you’re saying I’m 100% gonna be diabetic\n",
      "Enjoy a weekend of eating ice cream 100% guilt free. (That's standard practice for wisdom teeth still, right?)\n",
      "100% dead as in I'm 100% killing her myself if I get the chance.\n",
      "Is it manipulation if it is 100% true, 100% genuine, and 100% good advice?\n",
      "I can't believe people are saying [NAME] should have been saved, she knew like 10% of the words\n",
      "> and the top 20% of wealthiest [NAME] own 92% of stock Well, I hope they had a wonderful December, then.\n",
      "I think OD needs a rework. Get rid of that crappy 50% passive and give him some sort of active or a proper passive ability.\n",
      "It's definitely alive. Not sure it's doing quite so well...for 99% of people anyway.\n",
      "Damn, you're right. I could've sworn I saw a 99% up there.\n",
      "Great. So increase prices 20% overall, give the waiters the extra money. I'm happy with that. 20% is a great tip.\n",
      "It would take something extraordinary for me to turn down a 20% raise.\n",
      "Pretty much does. SNP polling at 41% for the next election, CON at 25, LAB 23, LIB 6, and Greens at 3.\n",
      "We will never survive with 5% unemployment with moderate growth and a continued house building program to correct supply. Sounds horrendous\n",
      "My anger, up 50%. \n",
      "Interesting read, thanks. Still a 40% bust rate. Sucks that someone's job is on the line with that high of a chance of failing. \n",
      "When the etiquette \"shitpost\" is 100% accurate\n",
      "Little did you know 95% of these kids just don’t want to go to school\n",
      "But you have about 0% chance of winning the lottery and the lottery isn't determined by your effort, skill, etc. \n",
      "But 80% of the time they go back to \"single\" (or just remove the relationship status), so no worries.\n",
      "I have a 1080ti and an i7 8700k and on very fast at 40k bitrate I'm already hitting 50% cpu usage on obs alone, any suggestions?\n",
      "Strangely enough 70% of women in the work place are queen bees\n",
      "At least you didn't open with, \"I was always a big [NAME] supporter...\" 99% of those go downhill fast.\n",
      "do you think sanders will be back at 100% after a torn achilles?\n",
      "Yes, I am 100% superior to [NAME]. If you're not, I'm sorry \n",
      "But if you go to fredmeyer, which is also Kroger, It's like 50% cheaper on most things. lol.\n",
      "30% of the country are insane.\n",
      "83% want it! Have a great weekend In solidarity, 4now\n",
      "This 100%. [NAME] was a great enforcer, but him scoring a hat trick with the Thrashers was just crazy.\n",
      "I think sweaty is hilarious in the context of r/forwardsfromgrandma which is where I see it 90% of the time\n",
      "Ya'll are way too hard on cops. Did you know that 60% of cops don't beat their spouse? That's more than half!\n",
      "a PG that shoots 41% on free throws? pass. would much rather have jrue if we're shitposting\n",
      "99.9999999% of sugar relationships involve sex. If you're not interested in sex, you should not sugar.\n",
      "23/68 last 9 games. Yeah that’s a bad %\n",
      "You don't get it, do you? There has already been a 51% attack attempt at BCH and the attackers lost. \n",
      "Well, its been hours and I was correct. The OPs comment was totally 100% incorrect. Have a great day, thanks for playing. LMAO!\n",
      "What makes you think any of it is hoarded? It’s likely to be near 100% invested.\n",
      "Oh, I’m so sorry! Good luck, I hope your 3000% more successful than me! Sending love and support!!!!!\n",
      "Even if it’s available 24/7 on your new website 99% of it is ugly so people still won’t buy it.\n",
      "5% more people identifying as liberal than conservative, 4.3% more people supporting the tax rates. [NAME] activated\n",
      "I'm 50% convinced this is [NAME] posting and I'm here for it\n",
      "Pareto principle, 20% of the work done gets you 80% results.\n",
      "Not surprising since about 85% of all dating relationships end in a break up.\n",
      "Lol- nah, she is pretty cool as far as that goes. She loves hosting people. 95% of her wardrobe is [NAME] though :(\n",
      "Yea gotta get that sweet .05% on $3,000. Gonna retire with it!\n",
      "I'm betting you're 100% right. 😊🍻\n",
      "I agree 100%\n",
      "Hurts donuts are dry. i love the toppings and suffer through the bottom 90% of the donut, really not worth it. \n",
      "Besides the fact that's it's funny she also looks like me witch makes it .5% funnier\n",
      "I assume you have have kids because you believe that since condoms are not 100% effective, you may as well not use them?\n",
      "I'm 100% sure you were punched in the face, but this was a good one\n",
      "I give either a book or music my full attention, not 50% :P\n",
      "the 28% /u/beelzebambi\n",
      "I would be impressed if [NAME] ate 15% of his weight every day I wish I lived in a state where the contest was valid. Stupid NY\n",
      "Why dont you go complaint about USA 75% deforestation?\n",
      "Downvote me into oblivion but both [NAME] and [NAME] shot around 35%. However one they say he balled out and the other one was struggling.\n",
      "They 100% know each other, maybe not personally\n",
      "It is 100% a scam. They've been copying many [NAME] with the same message.\n",
      "Just got back from TWC, and they were great! Thank you. 90% they were fried. Will try these others next \n",
      "Yeah lol. 25% of women are sexually assaulted in [RELIGION] countries, maybe. \n",
      "Don't know what more to say then. Your fridge full of beers that are all around 6% ABV says it all.\n",
      "Yep, all the mentally light-lifting folks will flock to that instead of her cautiously pushed out 70% tax rate.\n",
      "[NAME] don’t make up only 2% of the population though. Edit: I mis-read. I’m sorry. Please ignore this comment.\n",
      "100% agreed. If wanting people to understand the meaning of my comment makes me a coward, then I don't wanna be fucking brave.\n",
      "6% of [NAME] is [NAME] so it wouldn't be a shock if he was of this community.\n",
      "100% yes. My sister's ex is an actual monster but my niece still sees his family.\n",
      "Good luck! The life ahead of you holds 1000% more promise than the one you are leaving behind. They caused this ... not you.\n",
      "78% of child decisions goes to the mom.\n",
      "If we backed a second referendum and the Lib Dems were at 10% I'd be pretty annoyed to be honest. Cheeky sods.\n",
      "Hmm, I'm 99% sure he says there's a call for her. Could be wrong \n",
      "100% agree. Double standards are ever present. Young girl is sexualised, the whole world goes nuts, but a young boy is somehow perfectly acceptable. It's disgusting. \n",
      "80%? You watch too much YouTube.\n",
      "It doesn't matter if 99% didn't vote for him, it only maters that he got 50% +1 of the seats.\n",
      "Pretty much, it’s ironic to think how quickly the parties would switch positions if the illegal immigrants were expected to vote 90% for [NAME].\n",
      "just like [NAME] was going to win by 90+%? I'm not going to believe it until I see it.\n",
      "But your comparing [NAME] to harden and curry. That’s how you know simply comparing 3 pt % doesn’t tell you the full story.\n",
      "My meme coins are starting to hit the 70% growth range from the beginning of the month. I'm feeling bullish.\n",
      "I don't like it but it also makes sense. If [NAME] headlined a deal with the Mariners for [NAME], I'm all for it. 10000%\n",
      "You will miss 100% of the shots you don’t take, you just gotta take a chance and see.\n",
      "I agree with that, that's really the only reason I play pk too. Probably the reason my win rate for her is 40%\n",
      "good. can't even describe just how terrible this entire thing was. just make 1000% sure this is the right guy.\n",
      "If Wacky Races kept on going it would have 100% adult viewers\n",
      "100% yes\n",
      "I love this sub. Always 5% toxic, 95% thought out properly \n",
      "0% But then again most of us on here aren't placing top 8 at majors so that could factor in\n",
      "Not [NAME]! He had 0.9% kick to selfs so he'll probably keep bunging them in from the goal square and not getting any stats.\n",
      "50% completion percentage and was losing 31-10 at the half. Congrats, he got garbage time yards/points when [NAME] was just icing the game. \n",
      "Pretty dangerous for women, too. 40% of cops are domestic abusers. Stay safe. \n",
      "Its 100% copy paste.\n",
      "Theres no way 88% did that grossness.\n",
      "Lol i thought the ratio was 35% or 1/3 of [NAME]. I think u got it wrong But yeh gtfo of bum fuk nowhere\n",
      "Normally don't like telling people the value of their opinion, but this opinion... Is 100% correct carry on \n",
      "You think 60% is shafting anyone? I think Tellius is sub 20%.\n",
      "I think that making someone an approved submitter might get around that. Not 100% sure though.\n",
      "Just curious, why do you think that? He has ~ 80% approval among registered [NAME] \n",
      "Trae making sure the traetors don't make fun of his 3pt%\n",
      "Only about 1/10 uses is appropriate. The other 90% of the time context makes it obvious and an otherwise good joke is ruined.\n",
      "How old are you? It sounds like 90% of the guys you know who are your age also don’t have their shit together\n",
      "picture isnt that clear but I am 80% sure they're fake. break one and see how it crumbles.\n",
      "I believe so. Not 100% but safe to assume since they a USA based company\n",
      "Probably one everytime I run out of the $1 million, 97% success rate? Worth the risk \n",
      "That’s is 100% the noise. Perfect!\n",
      "100% scam\n",
      "Nope. A mamzer is someone who was born from an adulterous affair between two [NAME]. You're 100% [NAME] though!\n",
      "That's still a fucking insane number. 70% is goddamn insanity, and the only reason you're alright with it is because you're unaffected.\n",
      "Except it destroyed 76% of America's scooby snack production. May Shagster have mercy on us all.\n",
      "Water bottle works 100%. Canteen im not sure\n",
      "In all honesty no slip shoes do actually prevent slips like 99% of the time. This guy just seems to be the unlucky minority.\n",
      "[NAME], like people making minimum wage are worried about 10% off at their Walmart.\n",
      "2.5%? That not so great for [NAME].\n",
      "100% feel for you. I may be exaggerating slightly but still it’s all these little disadvantages that we get put at because of this ping.\n",
      "Well, if 10% of active gun owners each convert 4 people, we win :-).\n",
      "100% rotation of troops.\n",
      "It's absolutely 100% abuse, as others have said you need to get the hell away from him.\n",
      "Great way to be late on 80% of your orders. I mean I didn't say it was impossible, but you put rating at risk\n",
      "I wish my phone was that accurate. It warns me I'm at 14%, then 10 minutes later it drops to 0% and shuts off.\n",
      "Under [NAME], in 2018, unemployment raised above 4% at least 6 TIMES---in ONE year! What exactly are the [NAME] supporting? 🤔\n",
      "Ironically, everyone else (except a v small%) are thinking the same, try keeping this in mind next time.. enjoy your day\n",
      "Don't you understand, the people affected are [shuffles deck] able to get a 0% loan until the shutdown is over.\n",
      "100% this is enough to make me want to watch\n",
      "Using the logic that 100% of couples cheat.\n",
      "More like 25% in this scenario. Precarious though not necessarily disastrous.\n",
      "Frankly, I'm continuously shocked that more than 35% of the population actually think he's doing a good job. \n",
      "What are the chances [NAME] is in that 10% at 32?\n",
      "It always comes down to not accepting who they are themselves so it's pretty much 100% of it.\n",
      "I have 100% went to Wendy’s more after realizing I’m attracted to their mascot thanks to sassy twitter memes.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "target_column = df['Text']\n",
    "\n",
    "non_alphanumeric_characters = {}\n",
    "\n",
    "    # Iterate over the values in the specific column\n",
    "for value in target_column:\n",
    "    #print(value)\n",
    "        \n",
    "    # Use a regular expression to find non-alphanumeric characters\n",
    "    ats = []\n",
    "    ats = re.findall(r\"%\", value)\n",
    "    characters = re.findall(r'[^a-zA-Z0-9\\s]', value)\n",
    "    for character in characters:\n",
    "        if character not in non_alphanumeric_characters:\n",
    "            non_alphanumeric_characters[character] = value\n",
    "\n",
    "    if ats != []:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0662c82-8c17-47c9-9b30-1906865c1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non-alphanumeric characters in the text:\")\n",
    "for ch, sent in non_alphanumeric_characters.items():\n",
    "    print(ch)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "145f4758-8f19-4a7e-9148-b8594932bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slash(text):\n",
    "    text = re.sub(r'(?<=\\s)[rR]/', '', text)\n",
    "    text = re.sub(r\"^(?:r/|R/)\", '', text)\n",
    "    text = re.sub(r'(\\w+)\\s*/\\s*(\\w+)', r'\\1 or \\2', text)\n",
    "    return text\n",
    "\n",
    "def process_more(text):\n",
    "    text = re.sub(r'\\+(?:\\d+)', ' more ', text)\n",
    "    return text\n",
    "\n",
    "def process_and(text):\n",
    "    text = re.sub(r'\\s*&\\s*', ' and ', text)\n",
    "    return text\n",
    "\n",
    "def specific_case(text):\n",
    "    text = re.sub(r'\\sbi\\s', ' bisexual ', text)\n",
    "    text = re.sub(r'9-1-1', '911', text)\n",
    "    text = re.sub(r'0-0-0-0-0-10-0-0-01-0-01-0-0-10-0-0', '', text)\n",
    "    text = re.sub(r'(\\d+)ish', r'\\1', text)\n",
    "    text = re.sub(r't@gged', 'tagged', text)\n",
    "    text = re.sub(r'@.@', '', text)\n",
    "    text = re.sub(r'🐇', 'rabbit', text)\n",
    "    text = re.sub(r'I\\'m', 'I am', text)\n",
    "    text = re.sub(r'≠', ' does not equal ', text)\n",
    "    text = re.sub(r'you[´\\']re', ' you are ', text)\n",
    "    text = re.sub(r\"🤰\", ' emoji ', text)\n",
    "    patterns = [r'\\(fæ-shē\\)', r'\\/ˈsatʌɪə\\/', r'\\/ˈteCHē\\/', r'\\/ˈbɪɡət\\/']\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    text = re.sub(r\"xıs :ɹǝʍsuɐ\", ' answer: six ', text)\n",
    "    text = re.sub(r\"\\[NAME\\]\", ' user ', text)\n",
    "    text = re.sub(r\"pathetic-ness\", ' patheticness ', text)\n",
    "    result = result.replace(\"ó\", \"o\")\n",
    "    result = result.replace(\"ñ\", \"n\")\n",
    "    result = result.replace(\"é\", \"e\")\n",
    "    result = result.replace(\"ň\", \"n\")\n",
    "    result = result.replace(\"Я\", \"r\")\n",
    "    result = result.replace(\"ø\", \"o\")\n",
    "    result = result.replace(\"á\", \"a\")\n",
    "    result = result.replace(\"ī\", \"i\")\n",
    "    result = result.replace(\"ï\", \"i\")\n",
    "    result = result.replace(\"🅱\", \"b\")\n",
    "    result = result.replace(\"ò\", \"o\")\n",
    "    result = result.replace(\"ā\", \"a\")\n",
    "    result = result.replace(\"ú\", \"u\")\n",
    "    result = result.replace(\"è\", \"e\")\n",
    "    result = result.replace(\"Á\", \"A\")\n",
    "    result = result.replace(\"ç\", \"c\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_money(text):\n",
    "    text = re.sub(r\"(\\d+)\\s*\\€\", r' \\1 euros ', text)\n",
    "    text = re.sub(r\"(\\d+)\\s*\\$\", r' \\1 dollars ', text)\n",
    "    return text\n",
    "\n",
    "def process_hyphen_decimal_places(text):\n",
    "    text = re.sub(r'(\\d+(?:\\.\\d*)?)\\s*-\\s*(\\d+(?:\\.\\d*)?)', r'\\1 to \\2', text)\n",
    "    return text\n",
    "\n",
    "def process_at(text):\n",
    "    text = re.sub(r'@(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'@', ' at ', text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5ab8ac83-612c-413a-ac6e-7a521a0153cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: /rThis is a test\n",
      "Matched: /RAnother test\n",
      "Not Matched: This is not /r a test\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example strings\n",
    "strings = [\"/rThis is a test\", \"/RAnother test\", \"This is not /r a test\"]\n",
    "\n",
    "# Regular expression pattern\n",
    "pattern = r\"^(?:/r|/R)\"\n",
    "\n",
    "# Check if the pattern matches at the start of each string\n",
    "for string in strings:\n",
    "    if re.search(pattern, string):\n",
    "        print(f\"Matched: {string}\")\n",
    "    else:\n",
    "        print(f\"Not Matched: {string}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e5d9100-5d8c-482e-b8ab-4e0b94703824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is or an advanced or AI model or method.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Input text containing words separated by forward slashes\n",
    "text = \"OpenAI is / an advanced / AI model/method.\"\n",
    "\n",
    "# Using the regex to replace \"/\" with \"to\" between words\n",
    "text = re.sub(r'(\\w+)\\s*/\\s*(\\w+)', r'\\1 or \\2', text)\n",
    "\n",
    "# Display the modified text\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e213a1e1-61b8-4e55-91f6-bbcc094ae603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range is 12 to 67 and another range: 0 to 1.5.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"The range is 12 - 67 and another range: 0 - 1.5.\"\n",
    "\n",
    "# Use re.sub() to replace hyphens with \"to\" for numbers with optional decimal places\n",
    "text = re.sub(r'(\\d+(?:\\.\\d*)?)\\s*-\\s*(\\d+(?:\\.\\d*)?)', r'\\1 to \\2', text)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f520a52c-ece6-488f-ae38-101c171fc5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 123, and that's 45 and 45678.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"This is 123ish, and that's 45ish and 45678ish.\"\n",
    "\n",
    "# Use re.sub() to replace 'ish' with an empty string\n",
    "text = re.sub(r'(\\d+)ish', r'\\1', text)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c463db34-255e-4368-9508-65b1585d9af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I paid  50 euros  for the item, and   100 euros  for the other one.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"I paid 50€ for the item, and  100   € for the other one.\"\n",
    "\n",
    "# Use re.sub() to replace Euro amounts with ' euros '\n",
    "text = re.sub(r\"(\\d+)\\s*\\€\", r' \\1 euros ', text)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "60743e6c-544e-4d72-b907-c334eb690c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love is great\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = '@love is great'\n",
    "\n",
    "# Use re.sub() to replace '@word' with what comes after '@'\n",
    "text = re.sub(r'@(\\w+)', r'\\1', text)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "017cc7e7-5352-40fd-9064-5afc61b427bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have +3 apples, and John has +5 oranges.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"I have +3 apples, and John has +5 oranges.\"\n",
    "\n",
    "# Use re.sub() to replace numbers with '+' with ' more '\n",
    "text = re.sub(r'(?<=\\d)\\+', ' more ', text)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "543ba8fb-68f0-4e75-959a-39ddfa801cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have  plus 3 apples, and John has 4 more 5 oranges.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"I have +3 apples, and John has 4+5 oranges.\"\n",
    "\n",
    "# Replace '+' after a digit with ' more '\n",
    "result = re.sub(r'(?<=\\d)\\+', ' more ', text)\n",
    "\n",
    "# Replace '+' after a space with ' plus '\n",
    "result = re.sub(r'(?<=\\s)\\+', ' plus ', result)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3660397-90cf-44a2-8a40-824e0591e63a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
