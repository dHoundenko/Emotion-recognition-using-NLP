{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion recognition using NLP: Affective Computing 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danila Goncharenko, 2303788\n",
    "\n",
    "Ana Ferreira, 2308587\n",
    "\n",
    "Luca Hustiuc, 2209104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 565/565 [00:00<?, ?B/s] \n",
      "Downloading:   9%|▉         | 46.2M/501M [00:07<00:38, 11.7MB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\SAS Master's\\AffectiveComputing\\Project\\Code\\Emotion-recognition-using-NLP\\template_main.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SAS%20Master%27s/AffectiveComputing/Project/Code/Emotion-recognition-using-NLP/template_main.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(new_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SAS%20Master%27s/AffectiveComputing/Project/Code/Emotion-recognition-using-NLP/template_main.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model_nm \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcardiffnlp/twitter-roberta-base\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SAS%20Master%27s/AffectiveComputing/Project/Code/Emotion-recognition-using-NLP/template_main.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m fill_mask \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mfill-mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49mmodel_nm, tokenizer\u001b[39m=\u001b[39;49mmodel_nm)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SAS%20Master%27s/AffectiveComputing/Project/Code/Emotion-recognition-using-NLP/template_main.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_nm)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SAS%20Master%27s/AffectiveComputing/Project/Code/Emotion-recognition-using-NLP/template_main.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_candidates\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\__init__.py:428\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    427\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 428\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    429\u001b[0m     model,\n\u001b[0;32m    430\u001b[0m     model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    431\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    432\u001b[0m     framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    433\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m    434\u001b[0m     task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    435\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    436\u001b[0m )\n\u001b[0;32m    438\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    440\u001b[0m load_tokenizer \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(model_config) \u001b[39min\u001b[39;00m TOKENIZER_MAPPING\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\base.py:136\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    131\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m     )\n\u001b[0;32m    135\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    137\u001b[0m     \u001b[39m# Stop loading on the first successful load.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:384\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    383\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 384\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    385\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    386\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    387\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    388\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:1253\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1244\u001b[0m     archive_file \u001b[39m=\u001b[39m hf_bucket_url(\n\u001b[0;32m   1245\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   1246\u001b[0m         filename\u001b[39m=\u001b[39mfilename,\n\u001b[0;32m   1247\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   1248\u001b[0m         mirror\u001b[39m=\u001b[39mmirror,\n\u001b[0;32m   1249\u001b[0m     )\n\u001b[0;32m   1251\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1252\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m-> 1253\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_path(\n\u001b[0;32m   1254\u001b[0m         archive_file,\n\u001b[0;32m   1255\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1256\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   1257\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1258\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   1259\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   1260\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1261\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   1262\u001b[0m     )\n\u001b[0;32m   1263\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1264\u001b[0m     logger\u001b[39m.\u001b[39merror(err)\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\file_utils.py:1370\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     local_files_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m   1369\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m-> 1370\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[0;32m   1371\u001b[0m         url_or_filename,\n\u001b[0;32m   1372\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1373\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   1374\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1375\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   1376\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   1377\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1378\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   1379\u001b[0m     )\n\u001b[0;32m   1380\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m   1381\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[0;32m   1382\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\file_utils.py:1633\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1630\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1631\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[39m{\u001b[39;00mtemp_file\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1633\u001b[0m     http_get(url_to_download, temp_file, proxies\u001b[39m=\u001b[39;49mproxies, resume_size\u001b[39m=\u001b[39;49mresume_size, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m   1635\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mcache_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1636\u001b[0m os\u001b[39m.\u001b[39mreplace(temp_file\u001b[39m.\u001b[39mname, cache_path)\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\file_utils.py:1492\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[0;32m   1483\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1484\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m   1485\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1486\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1490\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logging\u001b[39m.\u001b[39mget_verbosity() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m   1491\u001b[0m )\n\u001b[1;32m-> 1492\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[0;32m   1493\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m   1494\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\utilizador.DESKTOP-DGA4C5V.000\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   9%|▉         | 46.6M/501M [00:18<00:38, 11.7MB/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, pipeline, AutoTokenizer\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "model_nm = \"cardiffnlp/twitter-roberta-base\"\n",
    "fill_mask = pipeline(\"fill-mask\", model=model_nm, tokenizer=model_nm)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_nm)\n",
    "\n",
    "def print_candidates():\n",
    "    for i in range(5):\n",
    "        token = tokenizer.decode(candidates[i]['token'])\n",
    "        score = np.round(candidates[i]['score'], 4)\n",
    "        print(f\"{i+1}) {token} {score}\")\n",
    "\n",
    "texts = [\n",
    " \"I am so <mask> 😊\",\n",
    " \"I am so <mask> 😢\" \n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    t = preprocess(text)\n",
    "    print(f\"{'-'*30}\\n{t}\")\n",
    "    candidates = fill_mask(t)\n",
    "    print_candidates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'ai-forever/ruBert-base'\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import numpy as np\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "text = \"Good night 😊\"\n",
    "text = preprocess(text)\n",
    "\n",
    "# Pytorch\n",
    "model = AutoModel.from_pretrained(MODEL)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "features = model(**encoded_input)\n",
    "features = features[0].detach().cpu().numpy() \n",
    "features_mean = np.mean(features[0], axis=0) \n",
    "#features_max = np.max(features[0], axis=0)\n",
    "\n",
    "# # Tensorflow\n",
    "# model = TFAutoModel.from_pretrained(MODEL)\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# features = model(encoded_input)\n",
    "# features = features[0].numpy()\n",
    "# features_mean = np.mean(features[0], axis=0) \n",
    "# #features_max = np.max(features[0], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 5.10k/5.10k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 3.67M/3.67M [00:01<00:00, 3.08MB/s]\n",
      "Downloading data: 100%|██████████| 447k/447k [00:00<00:00, 1.89MB/s]]\n",
      "Downloading data: 100%|██████████| 456k/456k [00:00<00:00, 925kB/s]s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  1.54it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 2710.08it/s]\n",
      "Generating train split: 26945 examples [00:00, 157760.06 examples/s]\n",
      "Generating validation split: 3294 examples [00:00, 131377.36 examples/s]\n",
      "Generating test split: 3367 examples [00:00, 136136.17 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Djacon/ru_goemotions\")\n",
    "\n",
    "def tok_func(x): return tokz(x['text'], truncation=True)\n",
    "\n",
    "LABELS = ['joy', 'interest', 'surpise', 'sadness', 'anger', 'disgust', 'fear', 'guilt', 'neutral']\n",
    "\n",
    "def binarize_labels(labels):\n",
    "    return [int(i in list(map(int,labels[1:-1].split(',')))) for i in range(len(LABELS))]\n",
    "binarize_labels(dataset['train']['labels'][0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# validate\n",
    "# test\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=len(LABELS), problem_type=\"multi_label_classification\")\n",
    "\n",
    "model.config.label2id = {label: i for i, label in enumerate(LABELS)}\n",
    "model.config.id2label = {i: label for i, label in enumerate(LABELS)}\n",
    "\n",
    "train_ds = dataset['train'].map(tok_func, batched=True).map(lambda x: {'label': [float(y) for y in binarize_labels(x['labels'])]}, batched=False, remove_columns=['text', 'labels'])\n",
    "val_ds = dataset['validation'].map(tok_func, batched=True).map(lambda x: {'label': [float(y) for y in binarize_labels(x['labels'])]}, batched=False, remove_columns=['text', 'labels'])\n",
    "test_ds = dataset['test'].map(lambda x: {'label': [float(y) for y in binarize_labels(x['labels'])]}, batched=False, remove_columns=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def get_pred(text):\n",
    "    inputs = tokz(text, return_tensors=\"pt\")\n",
    "    inputs.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    percent = nn.functional.softmax(logits, dim=1)\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    res = model.config.id2label[predicted_class_id]\n",
    "    return percent, predicted_class_id, res\n",
    "get_pred('This text has no emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "def predict_with_model(model, dataloader):\n",
    "    preds = []\n",
    "    facts = []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        facts.append([list(map(bool,batch['label']))])\n",
    "        inputs = tokz(batch['text'], return_tensors=\"pt\")\n",
    "        inputs.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        preds.append(nn.functional.softmax(logits, dim=1).cpu())\n",
    "    facts = np.concatenate(facts)\n",
    "    preds = np.concatenate(preds)\n",
    "    return facts, preds\n",
    "def eval_model(preds, facts):\n",
    "    aucs = [roc_auc_score(facts[:, i], preds[:, i]) for i in range(len(LABELS))]\n",
    "    print('aucs:', aucs)\n",
    "    return {'accuracy': np.mean(aucs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factswm, predwm = predict_with_model(model, test_ds.select(range(100)))\n",
    "eval_model(predwm, factswm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds, facts = eval_pred\n",
    "    return eval_model(preds, facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5, \n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\", \n",
    "    metric_for_best_model = \"accuracy\", \n",
    "    load_best_model_at_end=True, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokz,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred('This text has no emotion.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred('Im afraid Ill fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred('Glad to see you!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred('I want to die')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred('OMG, so romantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "pd.DataFrame([\n",
    "    {av: f1_score(factswm[:, i], predwm[:, i] > 0.5, average=av) for av in ['binary', 'micro', 'macro']}\n",
    "    for i in range(len(LABELS))\n",
    "]).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    {av: f1_score(factswm[:, i], predwm[:, i] > 0.5, average=av) for av in ['binary', 'micro', 'macro']}\n",
    "    for i in range(len(LABELS))\n",
    "]).mean().round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
