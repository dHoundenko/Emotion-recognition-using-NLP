{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cf513f-a6de-41f0-91be-18c210bebe82",
   "metadata": {},
   "source": [
    "In this notebook we will process the files inside the folder DatasetsInUse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab7b87-06ef-4f2c-8e51-c99f4838b605",
   "metadata": {},
   "source": [
    "First, we will start to process the data from the emotion_tweets_2020 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "090c1c72-3e15-455a-8d4c-98cbfc3fb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def mapping(key_list):\n",
    "    emotion_map = {\n",
    "        '0': \"anger\",\n",
    "        '1': \"joy\",\n",
    "        '2': \"optimism\",\n",
    "        '3': \"sadness\",\n",
    "    }\n",
    "\n",
    "    emotions = [emotion_map[num] for num in key_list]\n",
    "\n",
    "    return emotions\n",
    "\n",
    "def specific_case(text):\n",
    "    result = re.sub(r'(&gt;){3}', 'is better than', text)\n",
    "    result = result.replace(\"szn\", \"season\")\n",
    "    result = re.sub(r'&[gl]t;?', '', result)\n",
    "    result = result.replace(\"ó\", \"o\")\n",
    "    result = result.replace(\"ñ\", \"n\")\n",
    "    result = result.replace(\"é\", \"e\")\n",
    "    return result\n",
    "\n",
    "def normalize_repeated_characters(text):\n",
    "    # Replace 3 or more consecutive characters with just one\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    return re.sub(r'@(\\w+)', r'\\1', text)\n",
    "\n",
    "def process_more_sign(text):\n",
    "    result = re.sub(r'\\s*user \\+', 'user', text)\n",
    "    result = re.sub(r'#\\++', '', result)\n",
    "    result = re.sub(r'(?<=\\d)\\+', ' more ', result)\n",
    "    result = re.sub(r'(?<=\\s)\\+', ' plus ', result)\n",
    "    result = re.sub(r'\\+1', ' plus one ', result)\n",
    "    return result\n",
    "\n",
    "def process_dollar(text):\n",
    "    result = re.sub(r'\\${2,}', 'cash', text)\n",
    "    pattern = r'\\$(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' dollars ', result)\n",
    "    result = re.sub(r'\\$*', '', result)\n",
    "    return result\n",
    "\n",
    "def process_euro(text):\n",
    "    pattern = r'\\€(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' euros ', text)\n",
    "    return result\n",
    "\n",
    "def process_pounds(text):\n",
    "    pattern = r'\\£(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' pounds ', text)\n",
    "    return result\n",
    "\n",
    "def process_percent(text):\n",
    "    pattern = r'(?:\\s+|\\d+(?:\\.\\d{0,2})?)%'\n",
    "    result = re.sub(pattern, lambda match: match.group(0).replace('%', ' percent '), text)\n",
    "    result = re.sub(r'%', '', result)\n",
    "    return result\n",
    "\n",
    "def process_equal(text):\n",
    "    result = re.sub(r'=', ' equals ', text)\n",
    "    return result\n",
    "\n",
    "def process_at(text):\n",
    "    result = re.sub(r'(?<=\\s)@(?=\\s)', ' at ', text)\n",
    "    return result\n",
    "\n",
    "def remove_newlines(text):\n",
    "    return re.sub(r'\\\\n', ' ', text)\n",
    "\n",
    "def process_amp(text):\n",
    "    return re.sub(r'&amp;?', ' and ', text)\n",
    "\n",
    "def process_hyphen(text):\n",
    "    return re.sub(r'(\\d+)\\s*-\\s*(\\d+)', r'\\1 to \\2', text)\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    p = inflect.engine()\n",
    "\n",
    "    number_pattern = r'(\\d+\\.\\d+|\\d+)'\n",
    "\n",
    "    numbers = re.findall(number_pattern, text)\n",
    "\n",
    "    for number in numbers:\n",
    "        word_representation = p.number_to_words(number)\n",
    "        text = re.sub(re.escape(number), word_representation, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clear_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join(lemmatizer.lemmatize(word, wordnet.VERB) for word in tokens)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join(word for word in tokens if word.lower() not in stop_words)\n",
    "\n",
    "def process_text(text):\n",
    "    text = specific_case(text)\n",
    "    text = remove_user_mentions(text)\n",
    "    text = process_more_sign(text)\n",
    "    text = process_dollar(text)\n",
    "    text = process_euro(text)\n",
    "    text = process_pounds(text)\n",
    "    text = process_percent(text)\n",
    "    text = process_hyphen(text)\n",
    "    text = process_equal(text)\n",
    "    text = process_at(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = process_amp(text)\n",
    "    text = replace_numbers_with_words(text)\n",
    "    text = normalize_repeated_characters(text)\n",
    "    text = clear_special_characters(text)\n",
    "    text = lowercase_text(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "def create_df(keys_file, values_file):\n",
    "    keys = read_file(keys_file)\n",
    "    keys = mapping(keys)    \n",
    "    values = read_file(values_file)\n",
    "\n",
    "    processed_values = [process_text(value) for value in values]\n",
    "\n",
    "    data_dict = {\n",
    "        \"text\": processed_values,\n",
    "        \"emotions\": keys\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "keys_file = \"DatasetsInUse/emotion_tweets_2020/train_labels.txt\"\n",
    "values_file = \"DatasetsInUse/emotion_tweets_2020/train_text.txt\"\n",
    "\n",
    "df1 = create_df(keys_file, values_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4e25c-17c3-4023-84c9-e3f5017afcac",
   "metadata": {},
   "source": [
    "Secondly, we will process the data from the emotion folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a15ec8ae-ebad-4c0e-9002-98f9ba08c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416809\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_data(file):\n",
    "    return pd.read_pickle(file)\n",
    "\n",
    "file = \"DatasetsInUse/emotion/merged_training.pkl\"\n",
    "df2 = get_data(file)\n",
    "print(df2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4464fc-8eac-4f53-aef9-b0a56938d14c",
   "metadata": {},
   "source": [
    "Lastly, we will analyse the data from the go_emotion folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52e6bb92-5bab-4d02-9c05-c5164ad0343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_slash_data(text):\n",
    "    text = re.sub(r'(?<=\\s)[rR]/', '', text)\n",
    "    text = re.sub(r\"^(?:r/|R/)\", '', text)\n",
    "    text = re.sub(r'(\\w+)\\s*/\\s*(\\w+)', r'\\1 or \\2', text)\n",
    "    return text\n",
    "\n",
    "def process_more_data(text):\n",
    "    text = re.sub(r'\\+(\\d+)', r' more \\1', text)\n",
    "    text = re.sub(r'(?<=\\d)\\+', ' more ', text)\n",
    "    text = re.sub(r'(?<=\\s)\\+', ' plus ', text)\n",
    "    return text\n",
    "\n",
    "def process_and_data(text):\n",
    "    text = re.sub(r'\\s*&\\s*', ' and ', text)\n",
    "    return text\n",
    "\n",
    "def specific_case_data(text):\n",
    "    text = re.sub(r'\\sbi\\s', ' bisexual ', text)\n",
    "    text = re.sub(r'9-1-1', '911', text)\n",
    "    text = re.sub(r'0-0-0-0-0-10-0-0-01-0-01-0-0-10-0-0', '', text)\n",
    "    text = re.sub(r'(\\d+)ish', r'\\1', text)\n",
    "    text = re.sub(r't@gged', 'tagged', text)\n",
    "    text = re.sub(r'@.@', '', text)\n",
    "    text = re.sub(r'🐇', 'rabbit', text)\n",
    "    text = re.sub(r'I\\'m', 'I am', text)\n",
    "    text = re.sub(r'≠', ' does not equal ', text)\n",
    "    text = re.sub(r'you[´\\']re', ' you are ', text)\n",
    "    text = re.sub(r\"🤰\", ' emoji ', text)\n",
    "    patterns = [r'\\(fæ-shē\\)', r'\\/ˈsatʌɪə\\/', r'\\/ˈteCHē\\/', r'\\/ˈbɪɡət\\/']\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    text = re.sub(r\"xıs :ɹǝʍsuɐ\", ' answer: six ', text)\n",
    "    text = re.sub(r\"\\[NAME\\]\", ' user ', text)\n",
    "    text = re.sub(r\"pathetic-ness\", ' patheticness ', text)\n",
    "    text = re.sub(r'999999999999999999999999999999999999999999999999999999999999999999999999999999999999999991000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001234567898765432345676543345678987654345678909876543234567898765432345678909876543234567898765432345678987654323456787654345676543456543456434543434343434323456765434567654323454323456543345678987654323456789876565656565656565656565656565454545654565454323456765432345678765456', ' very big ', text)\n",
    "    text = text.replace(\"ó\", \"o\")\n",
    "    text = text.replace(\"ñ\", \"n\")\n",
    "    text = text.replace(\"é\", \"e\")\n",
    "    text = text.replace(\"ň\", \"n\")\n",
    "    text = text.replace(\"Я\", \"r\")\n",
    "    text = text.replace(\"ø\", \"o\")\n",
    "    text = text.replace(\"á\", \"a\")\n",
    "    text = text.replace(\"ī\", \"i\")\n",
    "    text = text.replace(\"ï\", \"i\")\n",
    "    text = text.replace(\"🅱\", \"b\")\n",
    "    text = text.replace(\"ò\", \"o\")\n",
    "    text = text.replace(\"ā\", \"a\")\n",
    "    text = text.replace(\"ú\", \"u\")\n",
    "    text = text.replace(\"è\", \"e\")\n",
    "    text = text.replace(\"Á\", \"A\")\n",
    "    text = text.replace(\"ç\", \"c\")\n",
    "    text = text.replace(\"abt\", \"about\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_money_data(text):\n",
    "    text = re.sub(r\"(\\d+)\\s*\\€\", r' \\1 euros ', text)\n",
    "    text = re.sub(r\"(\\d+)\\s*\\$\", r' \\1 dollars ', text)\n",
    "    return text\n",
    "\n",
    "def process_hyphen_decimal_places(text):\n",
    "    text = re.sub(r'(\\d+(?:\\.\\d*)?)\\s*-\\s*(\\d+(?:\\.\\d*)?)', r'\\1 to \\2', text)\n",
    "    return text\n",
    "\n",
    "def process_at_data(text):\n",
    "    text = re.sub(r'@(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'@', ' at ', text)\n",
    "    return text\n",
    "\n",
    "def length_word_bigger_one(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    filtered_words = [word for word in words if len(word) > 1]  # Filter words\n",
    "    return ' '.join(filtered_words)  # Join filtered words back into a sentence\n",
    "\n",
    "def final_processing(text):\n",
    "    #print(text)\n",
    "    text = specific_case_data(text)\n",
    "    text = process_slash_data(text)\n",
    "    text = process_more_data(text)\n",
    "    text = process_dollar(text)\n",
    "    text = process_euro(text)\n",
    "    text = process_pounds(text)\n",
    "    text = process_money_data(text)\n",
    "    text = process_percent(text)\n",
    "    text = process_hyphen_decimal_places(text)\n",
    "    text = process_equal(text)\n",
    "    text = process_at_data(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = process_amp(text)\n",
    "    text = process_and_data(text)\n",
    "    text = replace_numbers_with_words(text)\n",
    "    text = normalize_repeated_characters(text)\n",
    "    text = clear_special_characters(text)\n",
    "    text = lowercase_text(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = length_word_bigger_one(text)\n",
    "    #print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9090934-0d66-4054-91e8-ee616224dd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>favourite food anything cook</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>everyone think hes laugh screw people instead ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fuck bayless isoing</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make feel threaten</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dirty southern wankers</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotions\n",
       "0                       favourite food anything cook  neutral\n",
       "1  everyone think hes laugh screw people instead ...  neutral\n",
       "2                                fuck bayless isoing    anger\n",
       "3                                 make feel threaten     fear\n",
       "4                             dirty southern wankers    anger"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# function to divide each emotion string into a list of numbers\n",
    "# after that, the numbers are mapped to the corresponding emotion in emotions.txt\n",
    "# then these emotions are grouped\n",
    "def emotion_mapping(emotions, emotion_map, ekman_map):\n",
    "    numbers_list = emotions.split(',')\n",
    "    #print(numbers_list)\n",
    "    emotions_list = [emotion_map[int(num)] for num in numbers_list]\n",
    "    #print(emotions_list)\n",
    "    # group the emotions\n",
    "    ekman_list = [value for emotion in emotions_list for keys, value in ekman_map.items() if emotion in keys]\n",
    "    duplicates = [(item, ekman_list.count(item)) for item in set(ekman_list) if ekman_list.count(item) > 1]\n",
    "    sorted_duplicates = sorted(duplicates, key=lambda x: x[1], reverse=True)\n",
    "    #print(ekman_list)\n",
    "    #print(sorted_duplicates)\n",
    "    if sorted_duplicates == []:\n",
    "        if ekman_list == []:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            if 'neutral' in ekman_list:\n",
    "                return 'neutral'\n",
    "            else:\n",
    "                return ekman_list[0]\n",
    "    else:\n",
    "        return sorted_duplicates[0][0]\n",
    "\n",
    "def read_goemotions(df_file):\n",
    "    column_names = ['text', 'emotions', 'ID']\n",
    "\n",
    "    df3 = pd.read_csv(df_file, delimiter='\\t', names=column_names)\n",
    "\n",
    "    df3 = df3.drop(columns=['ID'])\n",
    "\n",
    "    # Reset the index\n",
    "    df3 = df3.reset_index(drop=True)\n",
    "\n",
    "    with open('DatasetsInUse/go_emotion/emotions.txt', 'r') as file:\n",
    "        # Read the contents of the file\n",
    "        file_contents = file.read()\n",
    "\n",
    "    lines = file_contents.split('\\n')\n",
    "\n",
    "    map = {index: line for index, line in enumerate(lines)}\n",
    "\n",
    "    file_path = 'DatasetsInUse/go_emotion/ekman_mapping.json'\n",
    "\n",
    "    try:\n",
    "        # Open the JSON file for reading\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            # Load and parse the JSON data\n",
    "            ekman_map = json.load(json_file)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f'JSON file not found: {file_path}')\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {str(e)}')\n",
    "\n",
    "    # Create a reverse lookup dictionary\n",
    "    ekman_map = {tuple(value): key for key, value in ekman_map.items()}\n",
    "\n",
    "    ekman_map[(\"neutral\", )] = \"neutral\"\n",
    "\n",
    "    df3['emotions'] = df3['emotions'].apply(emotion_mapping, args = (map, ekman_map))\n",
    "    df3['text'] = df3['text'].apply(final_processing)\n",
    "\n",
    "    return df3\n",
    "\n",
    "df_file = 'DatasetsInUse/go_emotion/train.tsv'\n",
    "df3 = read_goemotions(df_file)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c906a8-f5f0-45ea-9ca2-deb4bae1e1f5",
   "metadata": {},
   "source": [
    "Finally, we will combine the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b08cab-3e91-4c9b-a566-24d56ce3763a",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ecd690ac-52b4-489d-b95a-83ea98302646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463476\n",
      "                                                text  emotions\n",
      "0  worry payment problem may never joyce meyer mo...  optimism\n",
      "1  roommate okay spell autocorrect terrible first...     anger\n",
      "2      cute atsu probably shy photos cherry help uwu       joy\n",
      "3  rooneys fuck untouchable fuck dreadful depay l...     anger\n",
      "4  pretty depress u hit pan ur favourite highlighter   sadness\n"
     ]
    }
   ],
   "source": [
    "def combine_df(df1, df2):\n",
    "    return pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df = combine_df(df1, df2)\n",
    "df = combine_df(df, df3)\n",
    "df = df.reset_index(drop=True)\n",
    "print(df.shape[0])\n",
    "print(df.head())\n",
    "# df.emotions.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19e209-58c3-42f6-affc-88463b60a0ca",
   "metadata": {},
   "source": [
    "Selecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9558440a-a75b-4311-9426-838c8debcc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotions\n",
       "joy         6000\n",
       "neutral     6000\n",
       "love        6000\n",
       "surprise    6000\n",
       "anger       6000\n",
       "sadness     6000\n",
       "fear        6000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ['joy', 'sadness', 'anger', 'fear', 'love', 'surprise', 'neutral']\n",
    "selected_data = []\n",
    "\n",
    "for emotion in class_names:\n",
    "    # Filter the data for the current class\n",
    "    class_data = df[df['emotions'] == emotion]\n",
    "    # Select 6000 samples for the current class\n",
    "    if len(class_data) >= 6000:\n",
    "        selected_data.append(class_data.head(6000))\n",
    "    else:\n",
    "        selected_data.append(class_data)\n",
    "\n",
    "# Concatenate the selected data frames\n",
    "\n",
    "df = pd.concat(selected_data)\n",
    "\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Check the emotion values count for each class\n",
    "\n",
    "df.emotions.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198536ed-7d74-464f-af98-c9da4d0c6c4d",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de07c45c-8a7e-4374-9bf9-978f253a543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the filename for the Pickle file\n",
    "file_name_pkl = 'clean_training_data.pkl'\n",
    "\n",
    "# Save the DataFrame to a Pickle file\n",
    "df.to_pickle(file_name_pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
