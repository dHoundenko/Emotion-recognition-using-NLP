{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9cf513f-a6de-41f0-91be-18c210bebe82",
   "metadata": {},
   "source": [
    "In this notebook we will process the files inside the folder DatasetsInUse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab7b87-06ef-4f2c-8e51-c99f4838b605",
   "metadata": {},
   "source": [
    "First, we will start to process the data from the emotion_tweets_2020 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "090c1c72-3e15-455a-8d4c-98cbfc3fb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def mapping(key_list):\n",
    "    emotion_map = {\n",
    "        '0': \"anger\",\n",
    "        '1': \"joy\",\n",
    "        '2': \"optimism\",\n",
    "        '3': \"sadness\",\n",
    "    }\n",
    "\n",
    "    emotions = [emotion_map[num] for num in key_list]\n",
    "\n",
    "    return emotions\n",
    "\n",
    "def specific_case(text):\n",
    "    result = re.sub(r'(&gt;){3}', 'is better than', text)\n",
    "    result = result.replace(\"szn\", \"season\")\n",
    "    result = re.sub(r'&[gl]t;?', '', result)\n",
    "    result = result.replace(\"ó\", \"o\")\n",
    "    result = result.replace(\"ñ\", \"n\")\n",
    "    result = result.replace(\"é\", \"e\")\n",
    "    return result\n",
    "\n",
    "def normalize_repeated_characters(text):\n",
    "    # Replace 3 or more consecutive characters with just one\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    return re.sub(r'@(\\w+)', r'\\1', text)\n",
    "\n",
    "def process_more_sign(text):\n",
    "    result = re.sub(r'\\s*user \\+', 'user', text)\n",
    "    result = re.sub(r'#\\++', '', result)\n",
    "    result = re.sub(r'(?<=\\d)\\+', ' more ', result)\n",
    "    result = re.sub(r'(?<=\\s)\\+', ' plus ', result)\n",
    "    result = re.sub(r'\\+1', ' plus one ', result)\n",
    "    return result\n",
    "\n",
    "def process_dollar(text):\n",
    "    result = re.sub(r'\\${2,}', 'cash', text)\n",
    "    pattern = r'\\$(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' dollars ', result)\n",
    "    result = re.sub(r'\\$*', '', result)\n",
    "    return result\n",
    "\n",
    "def process_euro(text):\n",
    "    pattern = r'\\€(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' euros ', text)\n",
    "    return result\n",
    "\n",
    "def process_pounds(text):\n",
    "    pattern = r'\\£(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' pounds ', text)\n",
    "    return result\n",
    "\n",
    "def process_percent(text):\n",
    "    pattern = r'(?:\\s+|\\d+(?:\\.\\d{0,2})?)%'\n",
    "    result = re.sub(pattern, lambda match: match.group(0).replace('%', ' percent '), text)\n",
    "    result = re.sub(r'%', '', result)\n",
    "    return result\n",
    "\n",
    "def process_equal(text):\n",
    "    result = re.sub(r'=', ' equals ', text)\n",
    "    return result\n",
    "\n",
    "def process_at(text):\n",
    "    result = re.sub(r'(?<=\\s)@(?=\\s)', ' at ', text)\n",
    "    return result\n",
    "\n",
    "def remove_newlines(text):\n",
    "    return re.sub(r'\\\\n', ' ', text)\n",
    "\n",
    "def process_amp(text):\n",
    "    return re.sub(r'&amp;?', ' and ', text)\n",
    "\n",
    "def process_hyphen(text):\n",
    "    return re.sub(r'(\\d+)\\s*-\\s*(\\d+)', r'\\1 to \\2', text)\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    p = inflect.engine()\n",
    "\n",
    "    number_pattern = r'(\\d+\\.\\d+|\\d+)'\n",
    "\n",
    "    numbers = re.findall(number_pattern, text)\n",
    "\n",
    "    for number in numbers:\n",
    "        word_representation = p.number_to_words(number)\n",
    "        text = re.sub(re.escape(number), word_representation, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clear_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join(lemmatizer.lemmatize(word, wordnet.VERB) for word in tokens)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join(word for word in tokens if word.lower() not in stop_words)\n",
    "\n",
    "def process_text(text):\n",
    "    text = specific_case(text)\n",
    "    text = remove_user_mentions(text)\n",
    "    text = process_more_sign(text)\n",
    "    text = process_dollar(text)\n",
    "    text = process_euro(text)\n",
    "    text = process_pounds(text)\n",
    "    text = process_percent(text)\n",
    "    text = process_hyphen(text)\n",
    "    text = process_equal(text)\n",
    "    text = process_at(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = process_amp(text)\n",
    "    text = replace_numbers_with_words(text)\n",
    "    text = normalize_repeated_characters(text)\n",
    "    text = clear_special_characters(text)\n",
    "    text = lowercase_text(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "def create_df(keys_file, values_file):\n",
    "    keys = read_file(keys_file)\n",
    "    keys = mapping(keys)    \n",
    "    values = read_file(values_file)\n",
    "\n",
    "    processed_values = [process_text(value) for value in values]\n",
    "\n",
    "    data_dict = {\n",
    "        \"text\": processed_values,\n",
    "        \"emotions\": keys\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "keys_file = \"DatasetsInUse/emotion_tweets_2020/train_labels.txt\"\n",
    "values_file = \"DatasetsInUse/emotion_tweets_2020/train_text.txt\"\n",
    "\n",
    "df1 = create_df(keys_file, values_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4e25c-17c3-4023-84c9-e3f5017afcac",
   "metadata": {},
   "source": [
    "Secondly, we will process the data from the emotion folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15ec8ae-ebad-4c0e-9002-98f9ba08c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416809\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_data(file):\n",
    "    return pd.read_pickle(file)\n",
    "\n",
    "file = \"DatasetsInUse/emotion/merged_training.pkl\"\n",
    "df2 = get_data(file)\n",
    "print(df2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c906a8-f5f0-45ea-9ca2-deb4bae1e1f5",
   "metadata": {},
   "source": [
    "Finally, we will combine the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd690ac-52b4-489d-b95a-83ea98302646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420066\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worry payment problem may never joyce meyer mo...</td>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roommate okay spell autocorrect terrible first...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cute atsu probably shy photos cherry help uwu</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rooneys fuck untouchable fuck dreadful depay l...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pretty depress u hit pan ur favourite highlighter</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotions\n",
       "0  worry payment problem may never joyce meyer mo...  optimism\n",
       "1  roommate okay spell autocorrect terrible first...     anger\n",
       "2      cute atsu probably shy photos cherry help uwu       joy\n",
       "3  rooneys fuck untouchable fuck dreadful depay l...     anger\n",
       "4  pretty depress u hit pan ur favourite highlighter   sadness"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_df(df1, df2):\n",
    "    return pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df = combine_df(df1, df2)\n",
    "print(df.shape[0])\n",
    "df.head()\n",
    "# df.emotions.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
