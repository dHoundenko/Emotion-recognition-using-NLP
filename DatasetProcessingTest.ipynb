{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a6acb3-5e7e-4a9c-83eb-4f7cc59987f2",
   "metadata": {},
   "source": [
    "In this notebook we will process the files inside the folder DatasetsInUse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614a627-0beb-4d5b-bcbe-e96b8aa6b94d",
   "metadata": {},
   "source": [
    "First, we will start to process the data from the emotion_tweets_2020 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "808100a6-f814-43f8-9c24-2d194d12c210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3257\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worry payment problem may never joyce meyer mo...</td>\n",
       "      <td>optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roommate okay spell autocorrect terrible first...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cute atsu probably shy photos cherry help uwu</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rooneys fuck untouchable fuck dreadful depay l...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pretty depress u hit pan ur favourite highlighter</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotions\n",
       "0  worry payment problem may never joyce meyer mo...  optimism\n",
       "1  roommate okay spell autocorrect terrible first...     anger\n",
       "2      cute atsu probably shy photos cherry help uwu       joy\n",
       "3  rooneys fuck untouchable fuck dreadful depay l...     anger\n",
       "4  pretty depress u hit pan ur favourite highlighter   sadness"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def mapping(key_list):\n",
    "    emotion_map = {\n",
    "        '0': \"anger\",\n",
    "        '1': \"joy\",\n",
    "        '2': \"optimism\",\n",
    "        '3': \"sadness\",\n",
    "    }\n",
    "\n",
    "    emotions = [emotion_map[num] for num in key_list]\n",
    "\n",
    "    return emotions\n",
    "\n",
    "def specific_case(text):\n",
    "    result = re.sub(r'(&gt;){3}', 'is better than', text)\n",
    "    result = result.replace(\"szn\", \"season\")\n",
    "    result = re.sub(r'&[gl]t;?', '', result)\n",
    "    result = result.replace(\"ó\", \"o\")\n",
    "    result = result.replace(\"ñ\", \"n\")\n",
    "    result = result.replace(\"é\", \"e\")\n",
    "    return result\n",
    "\n",
    "def normalize_repeated_characters(text):\n",
    "    # Replace 3 or more consecutive characters with just one\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    return re.sub(r'@(\\w+)', r'\\1', text)\n",
    "\n",
    "def process_more_sign(text):\n",
    "    result = re.sub(r'\\s*user \\+', 'user', text)\n",
    "    result = re.sub(r'#\\++', '', result)\n",
    "    result = re.sub(r'(?<=\\d)\\+', ' more ', result)\n",
    "    result = re.sub(r'(?<=\\s)\\+', ' plus ', result)\n",
    "    result = re.sub(r'\\+1', ' plus one ', result)\n",
    "    return result\n",
    "\n",
    "def process_dollar(text):\n",
    "    result = re.sub(r'\\${2,}', 'cash', text)\n",
    "    pattern = r'\\$(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' dollars ', result)\n",
    "    result = re.sub(r'\\$*', '', result)\n",
    "    return result\n",
    "\n",
    "def process_euro(text):\n",
    "    pattern = r'\\€(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' euros ', text)\n",
    "    return result\n",
    "\n",
    "def process_pounds(text):\n",
    "    pattern = r'\\£(\\d+(?:\\.\\d{2})?)'\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' pounds ', text)\n",
    "    return result\n",
    "\n",
    "def process_percent(text):\n",
    "    pattern = r'(?:\\s+|\\d+(?:\\.\\d{0,2})?)%'\n",
    "    result = re.sub(pattern, lambda match: match.group(0).replace('%', ' percent '), text)\n",
    "    result = re.sub(r'%', '', result)\n",
    "    return result\n",
    "\n",
    "def process_equal(text):\n",
    "    result = re.sub(r'=', ' equals ', text)\n",
    "    return result\n",
    "\n",
    "def process_at(text):\n",
    "    result = re.sub(r'(?<=\\s)@(?=\\s)', ' at ', text)\n",
    "    return result\n",
    "\n",
    "def remove_newlines(text):\n",
    "    return re.sub(r'\\\\n', ' ', text)\n",
    "\n",
    "def process_amp(text):\n",
    "    return re.sub(r'&amp;?', ' and ', text)\n",
    "\n",
    "def process_hyphen(text):\n",
    "    return re.sub(r'(\\d+)\\s*-\\s*(\\d+)', r'\\1 to \\2', text)\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    p = inflect.engine()\n",
    "\n",
    "    number_pattern = r'(\\d+\\.\\d+|\\d+)'\n",
    "\n",
    "    numbers = re.findall(number_pattern, text)\n",
    "\n",
    "    for number in numbers:\n",
    "        word_representation = p.number_to_words(number)\n",
    "        text = re.sub(re.escape(number), word_representation, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clear_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join(lemmatizer.lemmatize(word, wordnet.VERB) for word in tokens)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join(word for word in tokens if word.lower() not in stop_words)\n",
    "\n",
    "def process_text(text):\n",
    "    text = specific_case(text)\n",
    "    text = remove_user_mentions(text)\n",
    "    text = process_more_sign(text)\n",
    "    text = process_dollar(text)\n",
    "    text = process_euro(text)\n",
    "    text = process_pounds(text)\n",
    "    text = process_percent(text)\n",
    "    text = process_hyphen(text)\n",
    "    text = process_equal(text)\n",
    "    text = process_at(text)\n",
    "    text = remove_newlines(text)\n",
    "    text = process_amp(text)\n",
    "    text = replace_numbers_with_words(text)\n",
    "    text = normalize_repeated_characters(text)\n",
    "    text = clear_special_characters(text)\n",
    "    text = lowercase_text(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "def create_df(keys_file, values_file):\n",
    "    keys = read_file(keys_file)\n",
    "    keys = mapping(keys)    \n",
    "    values = read_file(values_file)\n",
    "\n",
    "    processed_values = [process_text(value) for value in values]\n",
    "\n",
    "    #for value, processed_value in zip(values, processed_values):\n",
    "        #print(f\"Original Value: {value} - Processed Value: {processed_value}\")\n",
    "\n",
    "    # Create a dictionary to store the data\n",
    "    data_dict = {\n",
    "        \"text\": processed_values,\n",
    "        \"emotions\": keys  # Use the processed values\n",
    "    }\n",
    "\n",
    "    # Create a Pandas DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    # df.head()\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_dictionary(keys_file, values_file):\n",
    "    keys = read_file(keys_file)\n",
    "    keys = mapping(keys)    \n",
    "    values = read_file(values_file)\n",
    "    \n",
    "    # print(keys[:5])\n",
    "\n",
    "    # print(values[2839])\n",
    "\n",
    "    if len(keys) != len(values):\n",
    "        print(\"Error: The number of keys and values does not match.\")\n",
    "        return None\n",
    "\n",
    "    non_alphanumeric_characters = {}\n",
    "    \n",
    "    for key, value in zip(keys, values):\n",
    "        \n",
    "        #if key == \"Optimism\":\n",
    "        #    print(value)\n",
    "        # Use a regular expression to find non-alphanumeric characters\n",
    "        #ats = []\n",
    "        #ats = re.findall(r\"(\\d)\\1{2,}\", value)\n",
    "        characters = re.findall(r'[^a-zA-Z0-9\\s]', value)\n",
    "        for character in characters:\n",
    "            if character not in non_alphanumeric_characters:\n",
    "                non_alphanumeric_characters[character] = value\n",
    "\n",
    "        #if ats != []:\n",
    "            #print(value)\n",
    "\n",
    "    print(\"Non-alphanumeric characters in the text:\")\n",
    "    for ch, sent in non_alphanumeric_characters.items():\n",
    "        print(ch)\n",
    "        print(sent)\n",
    "\n",
    "    return emotion_tweets\n",
    "\n",
    "keys_file = \"DatasetsInUse/emotion_tweets_2020/train_labels.txt\"\n",
    "values_file = \"DatasetsInUse/emotion_tweets_2020/train_text.txt\"\n",
    "\n",
    "df = create_df(keys_file, values_file)\n",
    "\n",
    "print(df.shape[0])\n",
    "\n",
    "df.head()\n",
    "\n",
    "# resulting_dict = create_dictionary(keys_file, values_file)\n",
    "\n",
    "#for key in list(resulting_dict.keys())[:3]:\n",
    "    #value = resulting_dict[key]\n",
    "    #print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1662d8-663f-48d8-a7bd-e4dc1d00b258",
   "metadata": {},
   "source": [
    "Secondly, we will process the data from the emotion folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "76d018a5-a7a9-4dfe-b00b-d9fed585b501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27383</th>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110083</th>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140764</th>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100071</th>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text emotions\n",
       "27383   i feel awful about it too because it s my job ...  sadness\n",
       "110083                              im alone i feel awful  sadness\n",
       "140764  ive probably mentioned this before but i reall...      joy\n",
       "100071           i was feeling a little low few days back  sadness\n",
       "2837    i beleive that i am much more sensitive to oth...     love"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"DatasetsInUse/emotion/merged_training.pkl\")\n",
    "\n",
    "non_alphanumeric_characters = {}\n",
    "\n",
    "for row in df.itertuples():\n",
    "    characters = re.findall(r'[^a-zA-Z0-9\\s]', row.text)\n",
    "    for character in characters:\n",
    "            if character not in non_alphanumeric_characters:\n",
    "                non_alphanumeric_characters[character] = row.text\n",
    "    #print(row.emotions, row.text)\n",
    "\n",
    "# print(\"Non-alphanumeric characters in the text:\")\n",
    "for ch, sent in non_alphanumeric_characters.items():\n",
    "    print(ch)\n",
    "    print(sent)\n",
    "\n",
    "df.emotions.unique()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16fdcd9d-4070-4cd9-b73a-e0f2021346f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so happy today!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_repeated_characters(text):\n",
    "    # Replace 3 or more consecutive characters with just one\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "# Example\n",
    "text = \"I'm sooooo happyyyy today!!!\"\n",
    "normalized_text = normalize_repeated_characters(text)\n",
    "\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ca9a2e0-51f0-44eb-a55f-1099b0d71356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey user, great post!\n"
     ]
    }
   ],
   "source": [
    "def remove_user_mentions(text):\n",
    "    return re.sub(r'@\\w+', 'user', text)\n",
    "\n",
    "# Example\n",
    "text = \"Hey @user, great post!\"\n",
    "cleaned_text = remove_user_mentions(text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7eb890eb-a60d-4c89-96a0-e82b0523f681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Twitter     5 NLP\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "# Example\n",
    "text = \"Hello, Twitter! £§£€5 #NLP\"\n",
    "cleaned_text = remove_special_characters(text)\n",
    "cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', cleaned_text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5c2d3401-7047-4df2-8f89-4b10b73c2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: bdjdb berbéu é jhdbé éhdhdh ggdéff\n",
      "Modified Text: bdjdb berbeu e jhdbe ehdhdh ggdeff\n"
     ]
    }
   ],
   "source": [
    "def replace_szn_with_season(text):\n",
    "    # Use the str.replace() method to replace \"szn\" with \"season\"\n",
    "    #result = text.replace(\"szn\", \"season\")\n",
    "    result = text.replace(\"é\", \"e\")\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "original_text = \"@user szn 3 &gt;&gt;&gt; szn 1 &gt;&gt;&gt; szn 2. Just to warn you. Don't let szn 2 discourage you. \"\n",
    "text = \"bdjdb berbéu é jhdbé éhdhdh ggdéff\"\n",
    "modified_text = replace_szn_with_season(text)\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Modified Text:\", modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c38eab88-7020-464c-89a7-4a42672bf01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: @user szn 3 &gt;&gt;&gt; szn 1 &gt;&gt;&gt; szn 2. Just to warn you. Don't let szn 2 discourage you. \n",
      "Modified Text: @user szn 3 is better than szn 1 is better than szn 2. Just to warn you. Don't let szn 2 discourage you. \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_is_better_than(text):\n",
    "    # Use re.sub to replace \"is better than\" with \">>>\"\n",
    "    result = re.sub(r'(&gt;){3}', 'is better than', text)\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "original_text = \"@user szn 3 &gt;&gt;&gt; szn 1 &gt;&gt;&gt; szn 2. Just to warn you. Don't let szn 2 discourage you. \"\n",
    "modified_text = replace_is_better_than(original_text)\n",
    "\n",
    "print(\"Original Text:\", original_text)\n",
    "print(\"Modified Text:\", modified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d6432d92-f13b-494d-a899-153c23922d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user account created\n",
      "user account created\n",
      "user account created\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_user_plus_at_beginning(text):\n",
    "    pattern = r'\\s*user \\+'\n",
    "    replacement = 'user'\n",
    "    result = re.sub(pattern, replacement, text)\n",
    "    return result\n",
    "\n",
    "# Examples\n",
    "text1 = ' user + account created'\n",
    "text2 = 'user + account created'\n",
    "text3 = '    user + account created'\n",
    "\n",
    "print(replace_user_plus_at_beginning(text1))  # 'user account created'\n",
    "print(replace_user_plus_at_beginning(text2))  # 'user+ account created'\n",
    "print(replace_user_plus_at_beginning(text3))  # 'user account created'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "848a23bb-aa3c-4e9b-81b1-feff75042478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$$', '$$$', '$$$$']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_more_than_one_dollar_signs(text):\n",
    "    pattern = r'\\${2,}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "# Example\n",
    "text = \"The price is $$10.00, $$$25.50, and $$$$50.75.\"\n",
    "result = find_more_than_one_dollar_signs(text)\n",
    "print(result)  # ['$$', '$$$', '$$$$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d312579e-eab1-480d-8aac-61bbefaf47c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price is 10.00 dollars, but the total is 50.75 dollars.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def add_dollars_to_numbers(text):\n",
    "    # Define a regular expression pattern to match a dollar sign followed by a number\n",
    "    pattern = r'\\$(\\d+(?:\\.\\d{2})?)'\n",
    "    \n",
    "    # Use re.sub with a lambda function to replace matched text\n",
    "    result = re.sub(pattern, lambda match: match.group(1) + ' dollars', text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example\n",
    "text = \"The price is $10.00, but the total is $50.75.\"\n",
    "result = add_dollars_to_numbers(text)\n",
    "print(result)  # \"The price is 10.00 dollars, but the total is 50.75 dollars.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "01c2a9d7-3b68-483d-a4ec-903eb8d07602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The interest rate is 5 percent , and the discount is 10.25  percent .\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_percentages_with_string(text):\n",
    "    pattern = r'(?:\\s+|\\d+(?:\\.\\d{0,2})?)%'\n",
    "    \n",
    "    # Use re.sub with a lambda function to replace matched text\n",
    "    result = re.sub(pattern, lambda match: match.group(0).replace('%', ' percent '), text)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example\n",
    "text = \"The interest rate is 5%, and the discount is 10.25 %.\"\n",
    "result = replace_percentages_with_string(text)\n",
    "print(result)  # \"The interest rate is 5 percent, and the discount is 10.25 percent.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8605fb8f-9ea3-4966-a33b-85f1ae88afb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' @ ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Send me an email at user@example.com. Mention me @ mention if you have questions.\"\n",
    "\n",
    "matches = re.findall(r'\\s@\\s', text)\n",
    "\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fc31c2f7-ba99-4c3d-9123-8493cf13462b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send me an email at userexample.com. Mention me mention if you have questions.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Send me an email at user@example.com. Mention me @mention if you have questions.\"\n",
    "\n",
    "# Use a capturing group to capture the words after \"@\" and replace the entire match\n",
    "result = re.sub(r'@(\\w+)', r'\\1', text)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cf8a93f4-fc50-431f-bab2-72464ea7f664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send me an email at user  at  example.com. Mention me  at  mention if you have questions.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def process_at(text):\n",
    "    result = re.sub(r'(?<=\\s)@(?=\\s)', ' at ', text)\n",
    "    # result = re.sub(r'(?:\\s)(@)(?:\\s)', lambda match: match.group(1).replace('@', ' at '), text)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "text = \"Send me an email at user @ example.com. Mention me @ mention if you have questions.\"\n",
    "\n",
    "result = process_at(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "848de315-c2f1-454b-bc89-bd6eaa51e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I be run in the park .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join(lemmatizer.lemmatize(word, wordnet.VERB) for word in tokens)\n",
    "\n",
    "# Example\n",
    "text = \"I am                running in        the           park               .\"\n",
    "lemmatized_text = lemmatize_text(text)\n",
    "\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b75c69b8-5109-4f7e-881a-5a7896d4e3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I AM RACHEL</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOU ARE OUT OF YOUR MIND</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LALALALA</td>\n",
       "      <td>Optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO MORE</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YAY</td>\n",
       "      <td>Joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       text  emotions\n",
       "0               I AM RACHEL     Anger\n",
       "1  YOU ARE OUT OF YOUR MIND       Joy\n",
       "2                  LALALALA  Optimism\n",
       "3                   NO MORE   Sadness\n",
       "4                       YAY       Joy"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "keys = [\"Anger\", \"Joy\", \"Optimism\", \"Sadness\", \"Joy\", \"Joy\"]\n",
    "values = [\"I am Rachel\", \"You are out of your mind\", \"Lalalala\", \"No more\", \"Yay\", \"Let's go!\"]\n",
    "\n",
    "# Define a function to process each element in the \"Values\" list\n",
    "def process_value(value):\n",
    "    # Apply your processing function here\n",
    "    return value.upper()  # For example, convert to uppercase\n",
    "\n",
    "# Apply the processing function to each element in the \"Values\" list\n",
    "processed_values = [process_value(value) for value in values]\n",
    "\n",
    "# Create a dictionary to store the data\n",
    "data_dict = {\n",
    "    \"text\": processed_values,\n",
    "    \"emotions\": keys  # Use the processed values\n",
    "}\n",
    "\n",
    "# Create a Pandas DataFrame from the dictionary\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1798a02-60cd-4e3a-9118-de27f2ff7aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
